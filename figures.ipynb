{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import multiprocess as mp\n",
    "import pickle\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from coin import COIN\n",
    "from environments import CustomMountainCarEnv\n",
    "from rl import QLearningAgent, COINQLearningAgent\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 1: Contextualisation and Clustering\n",
    "\n",
    "## Plot effect of contextual policies on a training environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set scale factors for testing\n",
    "sf = np.concatenate([\n",
    "    0.0*np.ones((5000, )), \n",
    "    1.0*np.ones((5000, )),\n",
    "    1.5*np.ones((5000, )),\n",
    "    0.5*np.ones((5000, )),\n",
    "    0.5*np.ones((5000, )),\n",
    "    1.5*np.ones((5000, )),\n",
    "    0.0*np.ones((5000, )),\n",
    "    1.0*np.ones((5000, )),\n",
    "    1.0*np.ones((5000, ))\n",
    "    ])\n",
    "\n",
    "train_basic = False # Set to True to train the models, otherwise load saved values\n",
    "train_coin = False\n",
    "train_oracle = False\n",
    "train_optimal = False\n",
    "\n",
    "MAX_CORES = 17\n",
    "N_REPS = 100 # Number of repetitions of each model for averaging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic no-context model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_rep(rep_id, sf):\n",
    "    \"\"\"\n",
    "    Runs one repetition of training across all amplitudes in sf.\n",
    "    Returns the list of rewards.\n",
    "    \"\"\"\n",
    "    # Imports for multiprocessing\n",
    "    from environments import CustomMountainCarEnv\n",
    "    from rl import QLearningAgent\n",
    "    # Create a fresh agent and environment inside each process\n",
    "\n",
    "    env = CustomMountainCarEnv(amplitude=1.0, render_mode=\"none\")\n",
    "    agent = QLearningAgent(\n",
    "        env=env,\n",
    "        num_position_bins=30,\n",
    "        num_velocity_bins=30,\n",
    "        alpha=0.1,\n",
    "        gamma=0.99,\n",
    "        epsilon=1.0,\n",
    "        epsilon_decay=0.999,\n",
    "        min_epsilon=0.01\n",
    "    )\n",
    "\n",
    "    rewards_for_this_rep = []\n",
    "    for i, amplitude in enumerate(sf):\n",
    "        # Create the environment for each amplitude\n",
    "        env = CustomMountainCarEnv(amplitude=amplitude, render_mode=\"none\")\n",
    "        \n",
    "        # Train the agent in the current context\n",
    "        training_reward = agent.train_step(env=env, max_steps_per_episode=200)\n",
    "        rewards_for_this_rep.append(training_reward)\n",
    "\n",
    "        # Reset the environment epsilon at a change in amplitude\n",
    "        if i > 0 and amplitude != sf[i - 1]:\n",
    "            agent.epsilon = 1.0\n",
    "\n",
    "        if (i + 1) % 500 == 0:\n",
    "            print(f\"Repetition {rep_id}, Amplitude {amplitude}: Training reward = {training_reward}\")\n",
    "\n",
    "    return rewards_for_this_rep\n",
    "\n",
    "REWARDS_PATH = \"models/fig1a_rewards_basic.npy\"\n",
    "if train_basic:\n",
    "    # Number of repetitions and number of parallel processes:\n",
    "    n_reps = N_REPS\n",
    "    n_processes = MAX_CORES\n",
    "        \n",
    "    # Create a Pool of workers\n",
    "    with mp.Pool(processes=n_processes) as pool:\n",
    "        # Map each repetition to the function run_single_rep\n",
    "        all_results = pool.starmap(run_single_rep, [(rep, sf) for rep in range(n_reps)])\n",
    "        \n",
    "    # all_results is a list of lists: one list of rewards for each repetition\n",
    "    # Convert to a NumPy array, then average over axis=0 to get mean reward per amplitude\n",
    "    all_results_array = np.array(all_results)  # shape: (n_reps, len(sf))\n",
    "    rewards_basic = np.mean(all_results_array, axis=0)\n",
    "\n",
    "    # Save results\n",
    "    np.save(REWARDS_PATH, rewards_basic)\n",
    "    print(f\"Training complete. Rewards saved to '{REWARDS_PATH}'.\")\n",
    "else:\n",
    "    # Load the saved results from the training\n",
    "    if os.path.exists(REWARDS_PATH):\n",
    "        rewards_basic = np.load(REWARDS_PATH)\n",
    "        print(f\"Loaded rewards from '{REWARDS_PATH}'.\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No saved rewards found at '{REWARDS_PATH}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COIN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COIN_PATH = \"models/fig1a_coin_model_out.npy\"\n",
    "\n",
    "coin_model = COIN(\n",
    "    sigma_sensory_noise = 0.003, \n",
    "    sigma_motor_noise = 0.00182,\n",
    ")\n",
    "\n",
    "if train_coin:\n",
    "    # COIN model takes a long time to run - we simplify calculation here by assuming that \n",
    "    # the scale factor stays the same every 50 episodes.\n",
    "    coin_model.perturbations = sf[::50]\n",
    "\n",
    "    output = coin_model.simulate_coin()\n",
    "\n",
    "    known_c_resp, novel_c_resp = coin_model.get_responsibilities(output)\n",
    "    p_contexts = np.concatenate([known_c_resp,novel_c_resp[:,None]],axis=-1) \n",
    "    p_contexts = np.repeat(p_contexts, repeats=50, axis=0)\n",
    "\n",
    "    # Ensure COIN probabilities are of the right form\n",
    "    # Novel context - should never have a NaN definition\n",
    "    p_temp = p_contexts.copy()\n",
    "    p_temp[np.isnan(p_temp)] = 0.0\n",
    "    p_contexts[:,-1] = 1 - np.sum(p_temp[:,:-1], axis=1)\n",
    "    \n",
    "    # Save coin model output\n",
    "    np.save(COIN_PATH, p_contexts)\n",
    "else:\n",
    "    # Load the saved results from the training\n",
    "    if os.path.exists(COIN_PATH):\n",
    "        p_contexts = np.load(COIN_PATH)\n",
    "        known_c_resp = p_contexts[:,:-1]\n",
    "        novel_c_resp = p_contexts[:,-1]\n",
    "        print(f\"Loaded probabilities from '{COIN_PATH}'.\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No saved probabilities found at '{COIN_PATH}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_rep(rep_id, sf, p_context):\n",
    "    \"\"\"\n",
    "    Runs one repetition of training across all amplitudes in sf.\n",
    "    Returns the list of rewards.\n",
    "    \"\"\"\n",
    "    # Imports for multiprocessing\n",
    "    from environments import CustomMountainCarEnv\n",
    "    from rl import COINQLearningAgent\n",
    "    # Create a fresh agent and environment inside each process\n",
    "\n",
    "    C = p_context.shape[1]-1\n",
    "\n",
    "    env = CustomMountainCarEnv(amplitude=1.0, render_mode=\"none\")\n",
    "    agent = COINQLearningAgent(\n",
    "        env=env,\n",
    "        max_contexts=C,\n",
    "        num_position_bins=30,\n",
    "        num_velocity_bins=30,\n",
    "        alpha=0.1,\n",
    "        gamma=0.99,\n",
    "        epsilon=1.0,\n",
    "        epsilon_decay=0.999\n",
    "    )\n",
    "\n",
    "    rewards_for_this_rep = []\n",
    "    for i, amplitude in enumerate(sf):\n",
    "        # Create the environment for each amplitude\n",
    "        env = CustomMountainCarEnv(amplitude=amplitude, render_mode=\"none\")\n",
    "        \n",
    "        # Train the agent in the current context\n",
    "        training_reward = agent.train_step(env=env, p_context=p_context[i,:], max_steps_per_episode=200)\n",
    "        rewards_for_this_rep.append(training_reward)\n",
    "\n",
    "        if (i + 1) % 500 == 0:\n",
    "            print(f\"Repetition {rep_id}, Amplitude {amplitude}: Training reward = {training_reward}\")\n",
    "\n",
    "    return rewards_for_this_rep\n",
    "\n",
    "REWARDS_PATH = \"models/fig1a_rewards_coin.npy\"\n",
    "if train_coin:\n",
    "    # Number of repetitions and number of parallel processes:\n",
    "    n_reps = N_REPS\n",
    "    n_processes = MAX_CORES\n",
    "        \n",
    "    # Create a Pool of workers\n",
    "    with mp.Pool(processes=n_processes) as pool:\n",
    "        # Map each repetition to the function run_single_rep\n",
    "        all_results = pool.starmap(run_single_rep, [(rep, sf, p_contexts) for rep in range(n_reps)])\n",
    "        \n",
    "    # all_results is a list of lists: one list of rewards for each repetition\n",
    "    # Convert to a NumPy array, then average over axis=0 to get mean reward per amplitude\n",
    "    all_results_array = np.array(all_results)  # shape: (n_reps, len(sf))\n",
    "    rewards_coin = np.mean(all_results_array, axis=0)\n",
    "\n",
    "    # Save results\n",
    "    np.save(REWARDS_PATH, rewards_coin)\n",
    "    print(f\"Training complete. Rewards saved to '{REWARDS_PATH}'.\")\n",
    "else:\n",
    "    # Load the saved results from the training\n",
    "    if os.path.exists(REWARDS_PATH):\n",
    "        rewards_coin = np.load(REWARDS_PATH)\n",
    "        print(f\"Loaded rewards from '{REWARDS_PATH}'.\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No saved rewards found at '{REWARDS_PATH}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oracle (True Context Known)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REWARDS_PATH = \"models/fig1a_rewards_oracle.npy\"\n",
    "\n",
    "# We use the COIN Q-learning framework, although with perfect context knowledge\n",
    "p_contexts_oracle = np.zeros_like(p_contexts)\n",
    "p_contexts_oracle[sf==0.0, 0] = 1.0\n",
    "p_contexts_oracle[sf==0.5, 1] = 1.0\n",
    "p_contexts_oracle[sf==1.0, 2] = 1.0\n",
    "p_contexts_oracle[sf==1.5, 3] = 1.0\n",
    "if train_oracle:\n",
    "    # Number of repetitions and number of parallel processes:\n",
    "    n_reps = N_REPS\n",
    "    n_processes = MAX_CORES\n",
    "        \n",
    "    # Create a Pool of workers\n",
    "    with mp.Pool(processes=n_processes) as pool:\n",
    "        # Map each repetition to the function run_single_rep\n",
    "        all_results = pool.starmap(run_single_rep, [(rep, sf, p_contexts_oracle) for rep in range(n_reps)])\n",
    "        \n",
    "    # all_results is a list of lists: one list of rewards for each repetition\n",
    "    # Convert to a NumPy array, then average over axis=0 to get mean reward per amplitude\n",
    "    all_results_array = np.array(all_results)  # shape: (n_reps, len(sf))\n",
    "    rewards_oracle = np.mean(all_results_array, axis=0)\n",
    "\n",
    "    # Save results\n",
    "    np.save(REWARDS_PATH, rewards_oracle)\n",
    "    print(f\"Training complete. Rewards saved to '{REWARDS_PATH}'.\")\n",
    "else:\n",
    "    # Load the saved results from the training\n",
    "    if os.path.exists(REWARDS_PATH):\n",
    "        rewards_oracle = np.load(REWARDS_PATH)\n",
    "        print(f\"Loaded rewards from '{REWARDS_PATH}'.\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No saved rewards found at '{REWARDS_PATH}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_optimal:\n",
    "    # Optimal model will train on the amplitude values for 30000 episodes\n",
    "    envs = [CustomMountainCarEnv(amplitude=amp, render_mode=\"none\") for amp in [0.0, 0.5, 1.0, 1.5]]\n",
    "    agents = [QLearningAgent(\n",
    "        env=envs[i],\n",
    "        num_position_bins=30,\n",
    "        num_velocity_bins=30,\n",
    "        alpha=0.1,\n",
    "        gamma=0.99,\n",
    "        epsilon=1.0,\n",
    "        epsilon_decay=0.9999,\n",
    "        min_epsilon=0.01\n",
    "    ) for i in range(4)]\n",
    "\n",
    "    rewards_optimal = []\n",
    "\n",
    "    for i in range(4):\n",
    "        # Create the MountainCar environment with the true amplitude\n",
    "        env = envs[i]\n",
    "\n",
    "        # Train the agent in the current context\n",
    "        agent = agents[i]\n",
    "        _, training_rewards = agent.train(\n",
    "            env=env,\n",
    "            max_steps_per_episode=200,\n",
    "            n_episodes=30000,\n",
    "        )\n",
    "\n",
    "        rewards_optimal.append(np.max(training_rewards))\n",
    "\n",
    "        # Print the average training reward every 500 episodes\n",
    "        print(f\"Agent {i+1}, Average Training reward: {np.mean(training_rewards[-1000:])}\")\n",
    "\n",
    "    for i in range(4):\n",
    "        # Create the MountainCar environment with the true amplitude\n",
    "        env = envs[i]\n",
    "\n",
    "        # Evaluate the agent in the current context\n",
    "        agent = agents[i]\n",
    "        evaluation_reward = agent.evaluate(\n",
    "            env=env,\n",
    "            max_steps_per_episode=200,\n",
    "            n_episodes=1,\n",
    "        )\n",
    "\n",
    "        # If evaluation_reward[0] > rewards_optimal[i], substitute:\n",
    "        if evaluation_reward[0] > rewards_optimal[i]:\n",
    "            rewards_optimal[i] = evaluation_reward[0]\n",
    "\n",
    "        # Save results\n",
    "        np.save(\"models/fig1a_rewards_optimal.npy\", rewards_optimal)\n",
    "        with open(\"models/fig1a_rewards_optimal.pkl\", \"wb\") as f:\n",
    "            pickle.dump(agents, f)\n",
    "else:\n",
    "    # Load the saved results from the training\n",
    "    if os.path.exists(\"models/fig1a_rewards_optimal.npy\"):\n",
    "        rewards_optimal = np.load(\"models/fig1a_rewards_optimal.npy\")\n",
    "        print(f\"Loaded rewards from 'models/fig1a_rewards_optimal.npy'.\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No saved rewards found at 'models/fig1a_rewards_optimal.npy'.\")\n",
    "    if os.path.exists(\"models/fig1a_rewards_optimal.pkl\"):\n",
    "        with open(\"models/fig1a_rewards_optimal.pkl\", \"rb\") as f:\n",
    "            agents = pickle.load(f)\n",
    "        print(f\"Loaded agents from 'models/fig1a_rewards_optimal.pkl'.\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No saved agents found at 'models/fig1a_rewards_optimal.pkl'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 1a Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate regret for each different contextual model - specific to our data (not generalizable), here only for plotting\n",
    "def calculate_regret(rewards, optimal_rewards, sf_assignment):\n",
    "    \"\"\"\n",
    "    Calculate the regret for our scale factor data. Scale the regret to allow for better comparison.\n",
    "    \"\"\"\n",
    "    regret = np.zeros_like(rewards)\n",
    "    for i in range(len(optimal_rewards)):\n",
    "        # Add the optimal reward for the corresponding amplitude\n",
    "        regret[sf_assignment == i] = (-rewards[sf_assignment==i] + optimal_rewards[i])/(200+optimal_rewards[i])  \n",
    "    return regret\n",
    "\n",
    "sf_assignment = np.zeros_like(sf, dtype=int)\n",
    "for i, amplitude in enumerate([0.0, 0.5, 1.0, 1.5]):\n",
    "    sf_assignment[sf == amplitude] = i\n",
    "regret_basic = calculate_regret(rewards_basic, rewards_optimal, sf_assignment)\n",
    "regret_coin = calculate_regret(rewards_coin, rewards_optimal, sf_assignment)\n",
    "regret_oracle = calculate_regret(rewards_oracle, rewards_optimal, sf_assignment)\n",
    "\n",
    "# Plot smooth versions of the regrets\n",
    "regret_basic_s = np.convolve(regret_basic,np.ones(50,)/50, mode='same')\n",
    "regret_coin_s = np.convolve(regret_coin,np.ones(50,)/50, mode='same')\n",
    "regret_oracle_s = np.convolve(regret_oracle,np.ones(50,)/50, mode='same')\n",
    "\n",
    "# Tableau color palette\n",
    "tableau_colors = plt.get_cmap(\"tab10\").colors\n",
    "color_basic = tableau_colors[0]   # blue\n",
    "color_coin = tableau_colors[1]    # orange\n",
    "color_oracle = tableau_colors[2]  # green\n",
    "\n",
    "# Font setup\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"Times New Roman\",\n",
    "    \"font.size\": 14,\n",
    "    \"axes.titlesize\": 16,\n",
    "    \"axes.labelsize\": 14,\n",
    "    \"legend.fontsize\": 13,\n",
    "    \"xtick.labelsize\": 12,\n",
    "    \"ytick.labelsize\": 12\n",
    "})\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6.5, 4))\n",
    "plt.plot(regret_basic_s, label='Basic', color=color_basic, linewidth=1.8)\n",
    "plt.plot(regret_coin_s, label='COIN', color=color_coin, linewidth=1.8, linestyle='--')\n",
    "plt.plot(regret_oracle_s, label='Oracle', color=color_oracle, linewidth=1.8, linestyle=':')\n",
    "\n",
    "plt.ylim(-0.0, 1.2)\n",
    "plt.yticks(np.arange(0, 1.1, 0.2), fontsize=12)\n",
    "xticks = np.arange(0, len(regret_basic_s)+1, 5000)\n",
    "plt.xticks(xticks)\n",
    "\n",
    "# Labels and legend\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Regret (Normalised)')\n",
    "plt.legend(\n",
    "    loc='upper center',\n",
    "    bbox_to_anchor=(0.5, 0.99),\n",
    "    ncol=3,\n",
    "    frameon=True,\n",
    "    fancybox=False,\n",
    "    facecolor='white',\n",
    "    edgecolor='black'\n",
    ")\n",
    "\n",
    "# Tidy up\n",
    "plt.tight_layout()\n",
    "# Hide every other label\n",
    "ax = plt.gca()\n",
    "for i, label in enumerate(ax.get_xticklabels()):\n",
    "    if int(label.get_text()) % 10000 != 0:\n",
    "        label.set_visible(False)\n",
    "plt.savefig('figures/fig1a_regret.svg', dpi=300, bbox_inches='tight')  # Save as PDF for high quality\n",
    "plt.show()\n",
    "\n",
    "# Parameter variation plot\n",
    "plt.figure(figsize=(6.5, 3))\n",
    "plt.plot(sf, color='firebrick', linewidth=2.2, label='Scale Factor (sf)')\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Scale Factor')\n",
    "\n",
    "plt.yticks(np.array([0.0, 1.0, 1.5, 0.5]), fontsize=12)\n",
    "xticks = np.arange(0, len(regret_basic_s)+1, 5000)\n",
    "plt.xticks(xticks)\n",
    "\n",
    "plt.tight_layout()\n",
    "ax = plt.gca()\n",
    "for i, label in enumerate(ax.get_xticklabels()):\n",
    "    if int(label.get_text()) % 10000 != 0:\n",
    "        label.set_visible(False)\n",
    "plt.savefig('figures/fig1a_parameter.svg', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# plt.figure(figsize=(10,6))\n",
    "# plt.plot(sf)\n",
    "\n",
    "# plt.figure(figsize=(10,6))\n",
    "# plt.plot(p_contexts[:,:-1])\n",
    "\n",
    "# plt.figure(figsize=(10,6))\n",
    "# plt.plot(p_contexts_oracle[:,:-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot different clustering models\n",
    "Set up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data to cluster\n",
    "data = np.concatenate([\n",
    "    0.0*np.ones((100,)),\n",
    "    1.0*np.ones((100,)),\n",
    "    -1.0*np.ones((100,)),\n",
    "    np.linspace(-1.0,1.0,100),\n",
    "    0.5*np.ones((100,)),\n",
    "    0.0*np.ones((100,)),\n",
    "])\n",
    "\n",
    "# Number of clusters (does not apply to COIN)\n",
    "K = 4\n",
    "\n",
    "# Figure sizes\n",
    "fig1bsize = (5.5, 2.0)\n",
    "\n",
    "# Font setup\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"Times New Roman\",\n",
    "    \"font.size\": 14,\n",
    "    \"axes.titlesize\": 16,\n",
    "    \"axes.labelsize\": 14,\n",
    "    \"legend.fontsize\": 13,\n",
    "    \"xtick.labelsize\": 12,\n",
    "    \"ytick.labelsize\": 12\n",
    "})\n",
    "\n",
    "# Plot variation\n",
    "plt.figure(figsize=fig1bsize)\n",
    "plt.plot(data, color='firebrick', linewidth=2.2, label='Scale Factor (sf)')\n",
    "\n",
    "plt.ylabel('Parameter')\n",
    "\n",
    "plt.yticks(np.array([-1.0, -0.5, 0.0, 0.5, 1.0]), fontsize=12)\n",
    "xticks = np.arange(0, len(data)+1, 50)\n",
    "plt.xticks(xticks)\n",
    "\n",
    "plt.tight_layout()\n",
    "ax = plt.gca()\n",
    "for i, label in enumerate(ax.get_xticklabels()):\n",
    "    if int(label.get_text()) % 100 != 0:\n",
    "        label.set_visible(False)\n",
    "plt.savefig('figures/fig1b_parameter.svg', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _kmeans_pp_init(x: np.ndarray, K: int, rng: np.random.Generator):\n",
    "    \"\"\"k‑means++ initialisation for 1‑D data (returns K starting centroids).\"\"\"\n",
    "    centroids = np.empty(K, dtype=x.dtype)\n",
    "    centroids[0] = rng.choice(x)\n",
    "\n",
    "    # Squared distances to the closest chosen centroid\n",
    "    d2 = (x - centroids[0]) ** 2\n",
    "\n",
    "    for k in range(1, K):\n",
    "        probs = d2 / d2.sum()\n",
    "        centroids[k] = rng.choice(x, p=probs)\n",
    "        d2 = np.minimum(d2, (x - centroids[k]) ** 2)\n",
    "\n",
    "    return centroids\n",
    "\n",
    "\n",
    "def kmeans_1d(\n",
    "    P,\n",
    "    K: int,\n",
    "    max_iter: int = 100,\n",
    "    tol: float = 1e-4,\n",
    "    seed: int | None = None,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Clusters 1‑D data P into K groups.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    P : array‑like shape (N,)\n",
    "        The data points (floats or ints).\n",
    "    K : int\n",
    "        Number of clusters.\n",
    "    max_iter : int\n",
    "        Maximum number of Expectation–Maximisation iterations.\n",
    "    tol : float\n",
    "        Convergence threshold on centroid shift.\n",
    "    seed : int or None\n",
    "        RNG seed for reproducibility.\n",
    "    verbose : bool\n",
    "        If True, prints loss and centroid shift each iteration.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    labels : ndarray shape (N,)\n",
    "        Cluster index (0..K‑1) for every point in P.\n",
    "    centroids : ndarray shape (K,)\n",
    "        Final centroid positions.\n",
    "    \"\"\"\n",
    "    # --- prepare data & RNG ---\n",
    "    x = np.asarray(P, dtype=float).ravel()\n",
    "    N = x.size\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # --- initial centroids ---\n",
    "    centroids = _kmeans_pp_init(x, K, rng)\n",
    "\n",
    "    for it in range(1, max_iter + 1):\n",
    "        # ----- E‑step: assign each point to nearest centroid -----\n",
    "        distances = np.abs(x[:, None] - centroids[None, :])  # (N, K)\n",
    "        labels = distances.argmin(axis=1)                   # (N,)\n",
    "\n",
    "        # ----- M‑step: recompute centroids (mean of assigned points) -----\n",
    "        new_centroids = np.empty_like(centroids)\n",
    "        for k in range(K):\n",
    "            points_k = x[labels == k]\n",
    "            # handle empty cluster ⇒ re‑initialise to random point\n",
    "            new_centroids[k] = points_k.mean() if points_k.size else rng.choice(x)\n",
    "\n",
    "        # ----- check convergence -----\n",
    "        shift = np.abs(new_centroids - centroids).max()\n",
    "        if verbose:\n",
    "            inertia = ((x - new_centroids[labels]) ** 2).sum()\n",
    "            print(f\"iter {it:03d}  inertia={inertia:,.3f}  shift={shift:.5f}\")\n",
    "        if shift < tol:\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "\n",
    "    return labels, centroids\n",
    "\n",
    "# Run k-means clustering\n",
    "labels_kmeans, centroids_kmeans = kmeans_1d(data, K=K, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure and font\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"Times New Roman\",\n",
    "    \"font.size\": 13,\n",
    "    \"axes.titlesize\": 14,\n",
    "    \"axes.labelsize\": 13,\n",
    "    \"legend.fontsize\": 12,\n",
    "    \"xtick.labelsize\": 11,\n",
    "    \"ytick.labelsize\": 11\n",
    "})\n",
    "\n",
    "def remap_labels_by_first_appearance(labels):\n",
    "    label_map = {}\n",
    "    new_labels = []\n",
    "    next_label = 1\n",
    "    for label in labels:\n",
    "        if label not in label_map:\n",
    "            label_map[label] = next_label\n",
    "            next_label += 1\n",
    "        new_labels.append(label_map[label])\n",
    "    return np.array(new_labels)\n",
    "\n",
    "# Remap labels\n",
    "labels_kmeans = remap_labels_by_first_appearance(labels_kmeans)\n",
    "\n",
    "# Sorted unique labels (in case k-means returned [2, 0, 1] etc.)\n",
    "unique_labels = np.unique(labels_kmeans)\n",
    "K = len(unique_labels)\n",
    "cmap = plt.get_cmap('tab10')\n",
    "label_to_color = {label: cmap(i) for i, label in enumerate(unique_labels)}\n",
    "\n",
    "# Colors for each point in correct label order\n",
    "point_colors = [label_to_color[label] for label in labels_kmeans]\n",
    "\n",
    "# Create figure\n",
    "plt.figure(figsize=fig1bsize)\n",
    "scatter = plt.scatter(\n",
    "    np.arange(data.size),\n",
    "    data,\n",
    "    c=point_colors,\n",
    "    s=60,\n",
    "    edgecolors='none'\n",
    ")\n",
    "\n",
    "plt.ylabel('Parameter')\n",
    "plt.yticks(np.array([-1.0, -0.5, 0.0, 0.5, 1.0]), fontsize=12)\n",
    "plt.xticks(np.arange(0, len(data)+1, 50))\n",
    "\n",
    "# Manual legend with synced colors\n",
    "handles = [\n",
    "    plt.Line2D([0], [0], marker='o', color='w', label=f'C{i+1}',\n",
    "               markerfacecolor=cmap(i), markeredgecolor='none', markersize=8)\n",
    "    for i in range(K)\n",
    "]\n",
    "\n",
    "plt.legend(\n",
    "    handles=handles,\n",
    "    title='Context',\n",
    "    loc='lower right',\n",
    "    ncol=2,\n",
    "    frameon=True,\n",
    "    framealpha=1.0,\n",
    "    facecolor='white',\n",
    "    edgecolor='black',\n",
    "    columnspacing=0.8\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "ax = plt.gca()\n",
    "for i, label in enumerate(ax.get_xticklabels()):\n",
    "    if int(label.get_text()) % 100 != 0:\n",
    "        label.set_visible(False)\n",
    "\n",
    "plt.savefig('figures/fig1b_kmeans.svg', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "from math import log, pi\n",
    "\n",
    "\n",
    "# ---------- utilities ---------------------------------------------------------\n",
    "def _log_gaussian(x, mu, var):\n",
    "    \"\"\"log 𝒩(x | μ, σ²) — vectorised over x.\"\"\"\n",
    "    return -0.5 * (np.log(2 * pi * var) + (x - mu) ** 2 / var)\n",
    "\n",
    "\n",
    "def _kmeans_pp_init(x, K, rng):\n",
    "    \"\"\"1‑D k‑means++ initial means (returns sorted for convenience).\"\"\"\n",
    "    means = np.empty(K, dtype=x.dtype)\n",
    "    means[0] = rng.choice(x)\n",
    "    d2 = (x - means[0]) ** 2\n",
    "    for k in range(1, K):\n",
    "        probs = d2 / d2.sum()\n",
    "        means[k] = rng.choice(x, p=probs)\n",
    "        d2 = np.minimum(d2, (x - means[k]) ** 2)\n",
    "    return np.sort(means)\n",
    "\n",
    "\n",
    "# ---------- deterministic‑annealing EM ---------------------------------------\n",
    "def gmm_1d_da(x, K, anneal_schedule, max_iter=60, tol=1e-5, rng=None, verbose=0):\n",
    "    \"\"\"\n",
    "    One run of EM with an annealing schedule β₁…β_M (0<β<=1).\n",
    "    Returns (logL, labels, π, μ, var).\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = default_rng()\n",
    "    N = x.size\n",
    "\n",
    "    # initial parameters -------------------------------------------------------\n",
    "    mu = _kmeans_pp_init(x, K, rng)\n",
    "    var = np.full(K, x.var() + 1e-6)\n",
    "    pi = np.full(K, 1.0 / K)\n",
    "\n",
    "    def e_step(beta):\n",
    "        # responsibilities γᵢₖ ∝ (πₖ 𝒩)ᵝ\n",
    "        log_prob = beta * (_log_gaussian(x[:, None], mu, var) + np.log(pi))\n",
    "        log_tot = log_prob.max(axis=1, keepdims=True) + np.log(\n",
    "            np.exp(log_prob - log_prob.max(axis=1, keepdims=True)).sum(axis=1, keepdims=True)\n",
    "        )\n",
    "        resp = np.exp(log_prob - log_tot)\n",
    "        return resp, log_tot.sum() / beta          # true log‑likelihood (β cancels)\n",
    "\n",
    "    # annealing loop -----------------------------------------------------------\n",
    "    prev_ll = -np.inf\n",
    "    for beta in anneal_schedule:\n",
    "        for _ in range(max_iter):\n",
    "            resp, ll = e_step(beta)\n",
    "            Nk = resp.sum(axis=0) + 1e-10\n",
    "            pi = Nk / N\n",
    "            mu = (resp.T @ x) / Nk\n",
    "            diff2 = (x[:, None] - mu) ** 2\n",
    "            var = (resp * diff2).sum(axis=0) / Nk\n",
    "            var = np.clip(var, 1e-6, None)\n",
    "\n",
    "            if ll - prev_ll < tol:\n",
    "                break\n",
    "            prev_ll = ll\n",
    "        if verbose:\n",
    "            print(f\"β={beta:.2f}  iter≤{max_iter}  logL={ll:.4f}\")\n",
    "\n",
    "    labels = resp.argmax(axis=1)\n",
    "    return ll, labels, pi, mu, var\n",
    "\n",
    "\n",
    "# ---------- multi‑start wrapper ----------------------------------------------\n",
    "def gmm_1d_robust(\n",
    "    P,\n",
    "    K: int,\n",
    "    restarts: int = 10,\n",
    "    anneal_schedule: list[float] | None = None,\n",
    "    max_iter: int = 60,\n",
    "    tol: float = 1e-5,\n",
    "    seed: int | None = None,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Robust GMM clustering.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    P                : 1‑D data (array‑like)\n",
    "    K                : number of components\n",
    "    restarts         : how many independent runs to try\n",
    "    anneal_schedule  : list of β values. e.g. [0.2,0.4,0.6,0.8,1.0].\n",
    "                       None → plain EM (β=1).\n",
    "    max_iter, tol    : inner EM settings\n",
    "    seed             : RNG seed\n",
    "    verbose          : print progress\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_labels, best_pi, best_mu, best_var\n",
    "    \"\"\"\n",
    "    x = np.asarray(P, dtype=float).ravel()\n",
    "    rng_master = default_rng(seed)\n",
    "\n",
    "    if anneal_schedule is None:\n",
    "        anneal_schedule = [1.0]                     # plain EM\n",
    "\n",
    "    best_ll = -np.inf\n",
    "    best_res = None\n",
    "\n",
    "    for r in range(restarts):\n",
    "        rng = default_rng(rng_master.integers(1 << 32))\n",
    "        ll, labels, pi, mu, var = gmm_1d_da(\n",
    "            x,\n",
    "            K,\n",
    "            anneal_schedule,\n",
    "            max_iter=max_iter,\n",
    "            tol=tol,\n",
    "            rng=rng,\n",
    "            verbose=(verbose and restarts > 1),\n",
    "        )\n",
    "        if verbose and restarts > 1:\n",
    "            print(f\"restart {r+1}/{restarts}  final logL={ll:.4f}\")\n",
    "\n",
    "        if ll > best_ll:\n",
    "            best_ll = ll\n",
    "            best_res = (labels, pi, mu, var)\n",
    "\n",
    "    if verbose and restarts > 1:\n",
    "        print(f\"best logL={best_ll:.4f}\")\n",
    "\n",
    "    return best_res  # unpack as needed\n",
    "\n",
    "\n",
    "# -------------------------- demo ---------------------------------------------\n",
    "rng = default_rng(23)\n",
    "labels_gmm, pi_gmm, mu_gmm, var_gmm  = gmm_1d_robust(\n",
    "    data,\n",
    "    K=K,\n",
    "    max_iter=2000,\n",
    "    restarts=10,\n",
    "    anneal_schedule=[1.0],\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Compute GMM likelihoods ===\n",
    "likel = np.vstack([\n",
    "    pi_gmm[k] * np.exp(_log_gaussian(data, mu_gmm[k], var_gmm[k]))\n",
    "    for k in range(K)\n",
    "])\n",
    "likel = likel / np.sum(likel, axis=0)\n",
    "\n",
    "# === Get hard labels from GMM (most likely component per point) ===\n",
    "label_gmm_raw = np.argmax(likel, axis=0)\n",
    "\n",
    "# === Remap labels by first appearance (starting from 1) ===\n",
    "def remap_labels_by_first_appearance(labels):\n",
    "    label_map = {}\n",
    "    new_labels = []\n",
    "    next_label = 1\n",
    "    for label in labels:\n",
    "        if label not in label_map:\n",
    "            label_map[label] = next_label\n",
    "            next_label += 1\n",
    "        new_labels.append(label_map[label])\n",
    "    return np.array(new_labels), label_map\n",
    "\n",
    "label_gmm, label_map = remap_labels_by_first_appearance(label_gmm_raw)\n",
    "\n",
    "# === Invert label_map to get permutation index ===\n",
    "# label_map: {original_label -> new_label}\n",
    "# We want perm[new_label - 1] = original_label\n",
    "perm = [None] * len(label_map)\n",
    "for original_label, new_label in label_map.items():\n",
    "    perm[new_label - 1] = original_label\n",
    "perm = np.array(perm)\n",
    "\n",
    "# === Permute `likel` so that its rows follow the remapped order ===\n",
    "likel = likel[perm, :]\n",
    "\n",
    "\n",
    "# Font and figure styling\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"Times New Roman\",\n",
    "    \"font.size\": 13,\n",
    "    \"axes.titlesize\": 14,\n",
    "    \"axes.labelsize\": 13,\n",
    "    \"legend.fontsize\": 12,\n",
    "    \"xtick.labelsize\": 11,\n",
    "    \"ytick.labelsize\": 11\n",
    "})\n",
    "\n",
    "# Start figure\n",
    "plt.figure(figsize=fig1bsize)\n",
    "cmap = plt.get_cmap('tab10')\n",
    "colors = [cmap(i) for i in range(K)]\n",
    "\n",
    "# Plot each component's likelihood\n",
    "for k in range(K):\n",
    "    plt.plot(likel[k, :], color=colors[k], linewidth=1.6)\n",
    "\n",
    "# Axis formatting\n",
    "plt.ylabel('Likelihood')\n",
    "plt.xticks(np.arange(0, likel.shape[1]+1, 50))\n",
    "plt.yticks(np.linspace(0, 1, 5))\n",
    "\n",
    "# Tight layout & clean x-tick labeling\n",
    "plt.tight_layout()\n",
    "ax = plt.gca()\n",
    "for i, label in enumerate(ax.get_xticklabels()):\n",
    "    if int(label.get_text()) % 100 != 0:\n",
    "        label.set_visible(False)\n",
    "\n",
    "plt.savefig('figures/fig1b_gmm.svg', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfoMax1DClusterer(nn.Module):\n",
    "    \"\"\"Small MLP that acts as a discriminator q_theta(y|x).\"\"\"\n",
    "    def __init__(self, K: int, hidden: int = 32):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, K)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.net(x)\n",
    "        return torch.softmax(logits, dim=1)      # q_theta(y|x)\n",
    "\n",
    "\n",
    "def mutual_information_loss(q_batch: torch.Tensor, eps: float = 1e-10):\n",
    "    \"\"\"\n",
    "    -I(X;Y) Estimator (negative for optimisation).\n",
    "    q_batch  : (B, K) distributions q_theta(y|x) for a minibatch.\n",
    "    \"\"\"\n",
    "    # Pseudo-counts of clusters in the batch\n",
    "    p_y = q_batch.mean(dim=0)                    # (K,)\n",
    "    H_y = -(p_y * (p_y + eps).log()).sum()\n",
    "\n",
    "    H_y_given_x = -(q_batch * (q_batch + eps).log()).sum(dim=1).mean()\n",
    "\n",
    "    return -(H_y - H_y_given_x)\n",
    "\n",
    "\n",
    "def fit_infomax(P, K=3, epochs=500, batch_size=128, lr=1e-3, seed=0,\n",
    "                verbose=True):\n",
    "    \"\"\"\n",
    "    Train the discriminator and develop clustering indices for each point.\n",
    "    P          : array‑like unidimensional (n_samples,)\n",
    "    K          : cluster count\n",
    "    epochs     : training iterations\n",
    "    batch_size : minibatch size\n",
    "    lr         : learning rate Adam\n",
    "    seed       : reproducibility\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # ----- Prepare data -----\n",
    "    x = np.asarray(P, dtype=np.float32).reshape(-1, 1)\n",
    "    dataset = TensorDataset(torch.tensor(x))\n",
    "    loader = DataLoader(dataset, batch_size=min(batch_size, len(x)),\n",
    "                        shuffle=True, drop_last=False)\n",
    "\n",
    "    # ----- Model and optimiser -----\n",
    "    model = InfoMax1DClusterer(K)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # ----- Training -----\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        for (batch,) in loader:\n",
    "            q = model(batch)\n",
    "            loss = mutual_information_loss(q)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        if verbose and epoch % max(1, epochs // 10) == 0:\n",
    "            print(f\"Epoch {epoch:>4}/{epochs},  –I(X;Y) ≈ {loss.item():.4f}\")\n",
    "\n",
    "    # ----- Assign final tags -----\n",
    "    with torch.no_grad():\n",
    "        q_all = model(torch.tensor(x))\n",
    "        labels = torch.argmax(q_all, dim=1).cpu().numpy()\n",
    "\n",
    "    return labels, model\n",
    "\n",
    "\n",
    "labels, model = fit_infomax(data, K=K, epochs=300)\n",
    "print(\"Assigned classes (first 20):\", labels[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain probabilities for each class\n",
    "x = np.asarray(data, dtype=np.float32).reshape(-1, 1)\n",
    "q_all = model(torch.Tensor(x))\n",
    "q_all = q_all.cpu().detach().numpy().transpose()\n",
    "\n",
    "label_mi_raw = np.argmax(q_all, axis=0)\n",
    "\n",
    "# === Remap labels by first appearance (starting from 1) ===\n",
    "def remap_labels_by_first_appearance(labels):\n",
    "    label_map = {}\n",
    "    new_labels = []\n",
    "    next_label = 1\n",
    "    for label in labels:\n",
    "        if label not in label_map:\n",
    "            label_map[label] = next_label\n",
    "            next_label += 1\n",
    "        new_labels.append(label_map[label])\n",
    "    return np.array(new_labels), label_map\n",
    "\n",
    "label_mi, label_map = remap_labels_by_first_appearance(label_mi_raw)\n",
    "\n",
    "# === Invert label_map to get permutation index ===\n",
    "# label_map: {original_label -> new_label}\n",
    "# We want perm[new_label - 1] = original_label\n",
    "perm = [None] * len(label_map)\n",
    "for original_label, new_label in label_map.items():\n",
    "    perm[new_label - 1] = original_label\n",
    "perm = np.array(perm)\n",
    "\n",
    "q_all = q_all[perm, :]\n",
    "\n",
    "\n",
    "# Font and figure styling\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"Times New Roman\",\n",
    "    \"font.size\": 13,\n",
    "    \"axes.titlesize\": 14,\n",
    "    \"axes.labelsize\": 13,\n",
    "    \"legend.fontsize\": 12,\n",
    "    \"xtick.labelsize\": 11,\n",
    "    \"ytick.labelsize\": 11\n",
    "})\n",
    "\n",
    "# Start figure\n",
    "plt.figure(figsize=fig1bsize)\n",
    "cmap = plt.get_cmap('tab10')\n",
    "colors = [cmap(i) for i in range(K)]\n",
    "\n",
    "# Plot each component's likelihood\n",
    "for k in range(K):\n",
    "    plt.plot(q_all[k, :], color=colors[k], linewidth=1.6)\n",
    "\n",
    "# Axis formatting\n",
    "plt.ylabel('Likelihood')\n",
    "plt.xticks(np.arange(0, likel.shape[1]+1, 50))\n",
    "plt.yticks(np.linspace(0, 1, 5))\n",
    "\n",
    "# Tight layout & clean x-tick labeling\n",
    "plt.tight_layout()\n",
    "ax = plt.gca()\n",
    "for i, label in enumerate(ax.get_xticklabels()):\n",
    "    if int(label.get_text()) % 100 != 0:\n",
    "        label.set_visible(False)\n",
    "\n",
    "plt.savefig('figures/fig1b_mi.svg', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use COIN to classify the data - here we use the retention model\n",
    "\n",
    "coin_model = COIN(\n",
    "    sigma_sensory_noise = 0.003, \n",
    "    sigma_motor_noise = 0.00182,\n",
    ")\n",
    "\n",
    "coin_model.perturbations = data\n",
    "\n",
    "output = coin_model.simulate_coin()\n",
    "\n",
    "known_c_resp, novel_c_resp = coin_model.get_responsibilities(output)\n",
    "p_contexts = np.concatenate([known_c_resp,novel_c_resp[:,None]],axis=-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coin_known = known_c_resp.transpose()\n",
    "label_coin_raw = np.argmax(coin_known, axis=0)\n",
    "\n",
    "# === Remap labels by first appearance (starting from 1) ===\n",
    "def remap_labels_by_first_appearance(labels):\n",
    "    label_map = {}\n",
    "    new_labels = []\n",
    "    next_label = 1\n",
    "    for label in labels:\n",
    "        if label not in label_map:\n",
    "            label_map[label] = next_label\n",
    "            next_label += 1\n",
    "        new_labels.append(label_map[label])\n",
    "    return np.array(new_labels), label_map\n",
    "\n",
    "label_coin, label_map = remap_labels_by_first_appearance(label_coin_raw)\n",
    "\n",
    "# === Invert label_map to get permutation index ===\n",
    "# label_map: {original_label -> new_label}\n",
    "# We want perm[new_label - 1] = original_label\n",
    "perm = [None] * len(label_map)\n",
    "for original_label, new_label in label_map.items():\n",
    "    perm[new_label - 1] = original_label\n",
    "perm = np.array(perm)\n",
    "\n",
    "# === Permute `likel` so that its rows follow the remapped order ===\n",
    "coin_known = coin_known[perm, :]\n",
    "\n",
    "\n",
    "# Font and figure styling\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"Times New Roman\",\n",
    "    \"font.size\": 13,\n",
    "    \"axes.titlesize\": 14,\n",
    "    \"axes.labelsize\": 13,\n",
    "    \"legend.fontsize\": 12,\n",
    "    \"xtick.labelsize\": 11,\n",
    "    \"ytick.labelsize\": 11\n",
    "})\n",
    "\n",
    "# Start figure\n",
    "plt.figure(figsize=fig1bsize)\n",
    "cmap = plt.get_cmap('tab10')\n",
    "colors = [cmap(i) for i in range(K+1)]\n",
    "\n",
    "# Plot each component's likelihood\n",
    "for k in range(K):\n",
    "    plt.plot(coin_known[k, :], color=colors[k], linewidth=1.6)\n",
    "\n",
    "# Plot COIN novel context line with label\n",
    "plt.plot(novel_c_resp, color=colors[-1], linewidth=1.6, linestyle='--', label='Novel\\nContext')\n",
    "\n",
    "# Add single-item compact legend at middle right\n",
    "plt.legend(\n",
    "    loc='center right',\n",
    "    frameon=True,\n",
    "    facecolor='white',\n",
    "    edgecolor='black',\n",
    "    handlelength=2.0\n",
    ")\n",
    "\n",
    "# Axis formatting\n",
    "plt.ylabel('Likelihood')\n",
    "plt.xlabel('Episode')\n",
    "plt.xticks(np.arange(0, likel.shape[1]+1, 50))\n",
    "plt.yticks(np.linspace(0, 1, 5))\n",
    "\n",
    "# Tight layout & clean x-tick labeling\n",
    "plt.tight_layout()\n",
    "ax = plt.gca()\n",
    "for i, label in enumerate(ax.get_xticklabels()):\n",
    "    if int(label.get_text()) % 100 != 0:\n",
    "        label.set_visible(False)\n",
    "\n",
    "plt.savefig('figures/fig1b_coin.svg', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 2: Model Comparison and Generalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Comparison between different contextual approaches on fixed dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_1 = np.concatenate([\n",
    "    0.0*np.ones((3,)),\n",
    "    1.0*np.ones((3,)),\n",
    "    0.0*np.ones((2,)),\n",
    "    1.0*np.ones((2,)),\n",
    "    1.5*np.ones((2,)),\n",
    "    0.0*np.ones((2,)),\n",
    "    1.5*np.ones((3,)),\n",
    "    1.0*np.ones((1,)),\n",
    "    0.5*np.ones((2,)),\n",
    "]) # First data for Mountain Car\n",
    "\n",
    "mc_1 = np.repeat(mc_1, 1000, axis=0)\n",
    "\n",
    "mc1_gen = np.linspace(0.0,1.5,20) # Data to test generalisation\n",
    "\n",
    "# Train or load\n",
    "train_mc1 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COIN - Mountain Car 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train COIN Model\n",
    "coin_model = COIN(\n",
    "    sigma_sensory_noise = 0.003, \n",
    "    sigma_motor_noise = 0.00182,\n",
    ")\n",
    "\n",
    "coin_model.perturbations = mc_1[::100]\n",
    "\n",
    "output = coin_model.simulate_coin()\n",
    "\n",
    "known_c_resp, novel_c_resp = coin_model.get_responsibilities(output)\n",
    "p_contexts = np.concatenate([known_c_resp,novel_c_resp[:,None]],axis=-1) \n",
    "\n",
    "# Ensure COIN probabilities are of the right form\n",
    "# Novel context - should never have a NaN definition\n",
    "p_temp = p_contexts.copy()\n",
    "p_temp[np.isnan(p_temp)] = 0.0\n",
    "p_contexts[:,-1] = 1 - np.sum(p_temp[:,:-1], axis=1) # This variable has size [T,C]. It should be used for TRAINING, passed into the p_context parameter\n",
    "\n",
    "\n",
    "pred_resp = coin_model.get_predicted_responsibilities(output, mc1_gen) #This has size [N,T,C] - for gen_val. Each evaluation takes a new value of size [N,C].\n",
    "\n",
    "# Ensure interpolation of probabilities back to appropriate size\n",
    "p_contexts = np.repeat(p_contexts, repeats=100, axis=0)\n",
    "pred_resp = np.repeat(pred_resp, repeats=100, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_rep(rep_id, sf, p_context, gen_val, pred_resp):\n",
    "    \"\"\"\n",
    "    Runs one repetition of training across all amplitudes in sf. After every training step, evaluate for every \n",
    "    amplitude in gen_val.\n",
    "    Returns the list of rewards for the evaluation steps.\n",
    "    \"\"\"\n",
    "    # Imports for multiprocessing\n",
    "    from environments import CustomMountainCarEnv\n",
    "    from rl import COINQLearningAgent\n",
    "    import numpy as np\n",
    "    from tqdm.notebook import tqdm\n",
    "    # Create a fresh agent and environment inside each process\n",
    "\n",
    "    C = p_context.shape[1]-1\n",
    "\n",
    "    env = CustomMountainCarEnv(amplitude=1.0, render_mode=\"none\")\n",
    "    agent = COINQLearningAgent(\n",
    "        env=env,\n",
    "        max_contexts=C,\n",
    "        num_position_bins=30,\n",
    "        num_velocity_bins=30,\n",
    "        alpha=0.1,\n",
    "        gamma=0.99,\n",
    "        epsilon=1.0,\n",
    "        epsilon_decay=0.999\n",
    "    )\n",
    "\n",
    "    rewards_for_this_rep = []\n",
    "    training_rewards_for_this_rep = []\n",
    "    evaluation_rewards_for_this_rep = []\n",
    "    for t, amplitude in tqdm(enumerate(sf), total=len(sf)):\n",
    "        # Create the environment for each amplitude\n",
    "        env = CustomMountainCarEnv(amplitude=amplitude, render_mode=\"none\")\n",
    "        \n",
    "        # Train the agent in the current context\n",
    "        training_reward = agent.train_step(env=env, p_context=p_context[t,:], max_steps_per_episode=200)\n",
    "        training_rewards_for_this_rep.append(training_reward)\n",
    "\n",
    "        eval_r = agent.evaluate(env, p_context=p_context[t,:], n_episodes=1)[0]\n",
    "        evaluation_rewards_for_this_rep.append(eval_r)\n",
    "          \n",
    "        # Evaluate for all the gen_val\n",
    "        evaluation_rewards = np.zeros((gen_val.size,))\n",
    "\n",
    "        if t==0 or t % 500 == 0:\n",
    "            for i in range(evaluation_rewards.size):\n",
    "                eval_amp = gen_val[i]\n",
    "                env = CustomMountainCarEnv(amplitude=eval_amp, render_mode=\"none\")\n",
    "\n",
    "                evaluation_rewards[i] = np.mean(agent.evaluate(env, pred_resp[i,t,:], 10, 200, True)[0])\n",
    "            rewards_for_this_rep.append(evaluation_rewards)\n",
    "\n",
    "    return np.stack(rewards_for_this_rep), np.stack(training_rewards_for_this_rep), np.stack(evaluation_rewards_for_this_rep)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REWARDS_PATH = 'models/fig2a_rewards_mc1_coin.npy'\n",
    "if train_mc1:\n",
    "    all_results = run_single_rep(1, mc_1, p_contexts, mc1_gen, pred_resp)\n",
    "\n",
    "    rewards_mc1_coin = all_results[0]\n",
    "\n",
    "    # Save results\n",
    "    np.save(REWARDS_PATH, rewards_mc1_coin)\n",
    "    print(f\"Training complete. Rewards saved to '{REWARDS_PATH}'.\")\n",
    "        \n",
    "else:\n",
    "    # Load the saved results from the training\n",
    "    if os.path.exists(REWARDS_PATH):\n",
    "        rewards_mc1_coin = np.load(REWARDS_PATH)\n",
    "        print(f\"Loaded rewards from '{REWARDS_PATH}'.\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No saved rewards found at '{REWARDS_PATH}'.\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"Times New Roman\",\n",
    "    \"font.size\": 19,\n",
    "    \"axes.titlesize\": 19,\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"legend.fontsize\": 12,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16\n",
    "})\n",
    "\n",
    "# Normalize data\n",
    "data = rewards_mc1_coin\n",
    "data = (data - data.min()) / (data.max() - data.min())\n",
    "N, B = data.shape\n",
    "\n",
    "# ── Mask out zeros ─────────────────────────────────────────────────\n",
    "masked_data = np.ma.masked_where(data == 0, data)\n",
    "\n",
    "# ── Create a modified colormap where masked values (==0) appear black ─\n",
    "cmap = plt.get_cmap('viridis')\n",
    "cmap_with_black = cmap.copy()\n",
    "cmap_with_black.set_bad(color='black')  # set color for masked values\n",
    "\n",
    "# ── Plot ────────────────────────────────────────────────────────────\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "cax = ax.imshow(masked_data,\n",
    "                origin='upper',\n",
    "                aspect='auto',\n",
    "                interpolation='nearest',\n",
    "                vmin=0,\n",
    "                vmax=1,\n",
    "                cmap=cmap_with_black)\n",
    "\n",
    "# ── Training data markers ───────────────────────────────────────────\n",
    "d = mc_1[::500]\n",
    "col_indices = np.rint((d - mc1_gen.min())/(mc1_gen.max() - mc1_gen.min())*(mc1_gen.size - 1)).astype(int)\n",
    "row_indices = np.arange(N)\n",
    "\n",
    "ax.scatter(col_indices, row_indices,\n",
    "           facecolors='red',\n",
    "           edgecolors='white',\n",
    "           s=50,\n",
    "           linewidths=2,\n",
    "           marker='o')\n",
    "\n",
    "# ── Colorbar & labels ───────────────────────────────────────────────\n",
    "fig.colorbar(cax, ax=ax, label='Return (Scaled)')\n",
    "ax.set_xlabel('Parameter Value')\n",
    "ax.set_ylabel('Episode')\n",
    "\n",
    "# ── Ticks ───────────────────────────────────────────────────────────\n",
    "ax.set_xticks(np.arange(0,B,2))\n",
    "ax.set_yticks(np.arange(0, N, 5))\n",
    "ax.set_xticklabels([f\"{float(v):.2f}\" for v in mc1_gen[::2]])\n",
    "ax.set_yticklabels(np.arange(0, len(mc_1), 2500))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/fig2a_coin_mc1.svg', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means: Mountain Car 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.clustering import kmeans_1d\n",
    "\n",
    "K = 4\n",
    "data = mc_1\n",
    "\n",
    "# Run k-means clustering\n",
    "labels_kmeans, centroids_kmeans = kmeans_1d(mc_1, K=K, verbose=False)\n",
    "\n",
    "# We use the COIN Q-learning framework, although with perfect context knowledge\n",
    "p_contexts_kmeans = np.zeros((data.shape[0],5))\n",
    "p_contexts_kmeans[labels_kmeans==0,0] = 1.0\n",
    "p_contexts_kmeans[labels_kmeans==1,1] = 1.0\n",
    "p_contexts_kmeans[labels_kmeans==2,2] = 1.0\n",
    "p_contexts_kmeans[labels_kmeans==3,3] = 1.0\n",
    "\n",
    "# Predictive responsibilities from k-means simply finds the closest context\n",
    "test_labels = np.argmin(np.abs(mc1_gen[:, None] - centroids_kmeans[None, :]), axis=1)\n",
    "\n",
    "pred_resp_k = np.zeros((test_labels.shape[0],5))\n",
    "pred_resp_k[test_labels==0,0] = 1.0\n",
    "pred_resp_k[test_labels==1,1] = 1.0\n",
    "pred_resp_k[test_labels==2,2] = 1.0\n",
    "pred_resp_k[test_labels==3,3] = 1.0\n",
    "\n",
    "pred_resp_k = pred_resp_k[:,None,:]\n",
    "pred_resp_k = np.repeat(pred_resp_k, data.shape[0], axis=1)\n",
    "\n",
    "REWARDS_PATH = 'models/fig2a_rewards_mc1_kmeans.npy'\n",
    "if train_mc1:\n",
    "    # Number of repetitions and number of parallel processes:\n",
    "    n_reps = 1\n",
    "    n_processes = 1\n",
    "        \n",
    "    # Run tests\n",
    "    all_results = run_single_rep(1, mc_1, p_contexts_kmeans, mc1_gen, pred_resp_k)\n",
    "\n",
    "    rewards_mc1_kmeans = all_results[0]\n",
    "\n",
    "    # Save results\n",
    "    np.save(REWARDS_PATH, rewards_mc1_kmeans)\n",
    "    print(f\"Training complete. Rewards saved to '{REWARDS_PATH}'.\")\n",
    "        \n",
    "else:\n",
    "    # Load the saved results from the training\n",
    "    if os.path.exists(REWARDS_PATH):\n",
    "        rewards_mc1_kmeans = np.load(REWARDS_PATH)\n",
    "        print(f\"Loaded rewards from '{REWARDS_PATH}'.\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No saved rewards found at '{REWARDS_PATH}'.\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"Times New Roman\",\n",
    "    \"font.size\": 19,\n",
    "    \"axes.titlesize\": 19,\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"legend.fontsize\": 12,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16\n",
    "})\n",
    "\n",
    "# ── Prepare data ──────────────────────────────────────────────────────\n",
    "data = rewards_mc1_kmeans\n",
    "data = (data - data.min()) / (data.max() - data.min())  # normalize\n",
    "N, B = data.shape\n",
    "\n",
    "# ── Mask zeros ────────────────────────────────────────────────────────\n",
    "masked_data = np.ma.masked_where(data == 0, data)\n",
    "\n",
    "# ── Use viridis colormap but set 'bad' values (masked) to black ───────\n",
    "cmap = plt.get_cmap('viridis').copy()\n",
    "cmap.set_bad(color='black')\n",
    "\n",
    "# ── Plot heatmap ──────────────────────────────────────────────────────\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "cax = ax.imshow(masked_data,\n",
    "                origin='upper',\n",
    "                aspect='auto',\n",
    "                interpolation='nearest',\n",
    "                vmin=0,\n",
    "                vmax=1,\n",
    "                cmap=cmap)\n",
    "\n",
    "# ── Highlight training data positions ─────────────────────────────────\n",
    "d = mc_1[::500]\n",
    "col_indices = np.rint((d - mc1_gen.min()) / (mc1_gen.max() - mc1_gen.min()) * (mc1_gen.size - 1)).astype(int)\n",
    "row_indices = np.arange(N)\n",
    "\n",
    "ax.scatter(col_indices, row_indices,\n",
    "           facecolors='red',\n",
    "           edgecolors='white',\n",
    "           s=50,\n",
    "           linewidths=2,\n",
    "           marker='o')\n",
    "\n",
    "# ── Labels, ticks, and colorbar ───────────────────────────────────────\n",
    "fig.colorbar(cax, ax=ax, label='Return (Scaled)')\n",
    "ax.set_xlabel('Parameter Value')\n",
    "ax.set_ylabel('Episode')\n",
    "\n",
    "ax.set_xticks(np.arange(0,B,2))\n",
    "ax.set_yticks(np.arange(0, N, 5))\n",
    "ax.set_xticklabels([f\"{float(v):.2f}\" for v in mc1_gen[::2]])\n",
    "ax.set_yticklabels(np.arange(0, len(mc_1), 2500))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/fig2a_kmeans_mc1.svg', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaffolding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data for scaffolding test\n",
    "mc_2 = np.concatenate([\n",
    "    np.ones((5,)),\n",
    "    np.linspace(1.0, 1.8, 20),\n",
    "])\n",
    "\n",
    "mc_2 = np.repeat(mc_2, 500, axis=0)\n",
    "\n",
    "eps_decay = 1/mc_2.size * np.log(1.0/0.01)\n",
    "\n",
    "mc2_gen = np.linspace(1.0,1.8,21) # Data to test generalisation\n",
    "\n",
    "# Train or load\n",
    "train_mc2 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COIN - Mountain Car 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train COIN Model\n",
    "coin_model = COIN(\n",
    "    sigma_sensory_noise = 0.003, \n",
    "    sigma_motor_noise = 0.00182,\n",
    ")\n",
    "\n",
    "coin_model.perturbations = mc_2[::100]\n",
    "\n",
    "output = coin_model.simulate_coin()\n",
    "\n",
    "known_c_resp, novel_c_resp = coin_model.get_responsibilities(output)\n",
    "p_contexts = np.concatenate([known_c_resp,novel_c_resp[:,None]],axis=-1) \n",
    "\n",
    "# Ensure COIN probabilities are of the right form\n",
    "# Novel context - should never have a NaN definition\n",
    "p_temp = p_contexts.copy()\n",
    "p_temp[np.isnan(p_temp)] = 0.0\n",
    "p_contexts[:,-1] = 1 - np.sum(p_temp[:,:-1], axis=1) # This variable has size [T,C]. It should be used for TRAINING, passed into the p_context parameter\n",
    "\n",
    "\n",
    "pred_resp = coin_model.get_predicted_responsibilities(output, mc2_gen) #This has size [N,T,C] - for gen_val. Each evaluation takes a new value of size [N,C].\n",
    "\n",
    "# Ensure interpolation of probabilities back to appropriate size\n",
    "p_contexts = np.repeat(p_contexts, repeats=100, axis=0)\n",
    "pred_resp = np.repeat(pred_resp, repeats=100, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_rep(rep_id, sf, p_context, gen_val, pred_resp):\n",
    "    \"\"\"\n",
    "    Runs one repetition of training across all amplitudes in sf. After every training step, evaluate for every \n",
    "    amplitude in gen_val.\n",
    "    Returns the list of rewards for the evaluation steps.\n",
    "    \"\"\"\n",
    "    # Imports for multiprocessing\n",
    "    from environments import CustomMountainCarEnv\n",
    "    from rl import COINQLearningAgent\n",
    "    import numpy as np\n",
    "    from tqdm.notebook import tqdm\n",
    "    # Create a fresh agent and environment inside each process\n",
    "\n",
    "    C = p_context.shape[1]-1\n",
    "\n",
    "    env = CustomMountainCarEnv(amplitude=1.0, render_mode=\"none\")\n",
    "    agent = COINQLearningAgent(\n",
    "        env=env,\n",
    "        max_contexts=C,\n",
    "        num_position_bins=30,\n",
    "        num_velocity_bins=30,\n",
    "        alpha=0.1,\n",
    "        gamma=0.99,\n",
    "        epsilon=1.0,\n",
    "        epsilon_decay=eps_decay\n",
    "    )\n",
    "\n",
    "    rewards_for_this_rep = []\n",
    "    training_rewards_for_this_rep = []\n",
    "    evaluation_rewards_for_this_rep = []\n",
    "    for t, amplitude in tqdm(enumerate(sf), total=len(sf)):\n",
    "        # Create the environment for each amplitude\n",
    "        env = CustomMountainCarEnv(amplitude=amplitude, render_mode=\"none\")\n",
    "        \n",
    "        # Train the agent in the current context\n",
    "        training_reward = agent.train_step(env=env, p_context=p_context[t,:], max_steps_per_episode=200)\n",
    "        training_rewards_for_this_rep.append(training_reward)\n",
    "\n",
    "        eval_r = agent.evaluate(env, p_context=p_context[t,:], n_episodes=1)[0]\n",
    "        evaluation_rewards_for_this_rep.append(eval_r)\n",
    "          \n",
    "        # Evaluate for all the gen_val\n",
    "        evaluation_rewards = np.zeros((gen_val.size,))\n",
    "\n",
    "        if t==0 or t % 500 == 0:\n",
    "            for i in range(evaluation_rewards.size):\n",
    "                eval_amp = gen_val[i]\n",
    "                env = CustomMountainCarEnv(amplitude=eval_amp, render_mode=\"none\")\n",
    "\n",
    "                evaluation_rewards[i] = np.mean(agent.evaluate(env, pred_resp[i,t,:], 10, 200, True)[0])\n",
    "            rewards_for_this_rep.append(evaluation_rewards)\n",
    "\n",
    "    return np.stack(rewards_for_this_rep), np.stack(training_rewards_for_this_rep), np.stack(evaluation_rewards_for_this_rep)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REWARDS_PATH = 'models/fig2a_rewards_mc2_coin.npy'\n",
    "if train_mc2:\n",
    "    all_results = run_single_rep(1, mc_2, p_contexts, mc2_gen, pred_resp)\n",
    "\n",
    "    rewards_mc2_coin = all_results[0]\n",
    "\n",
    "    # Save results\n",
    "    np.save(REWARDS_PATH, rewards_mc2_coin)\n",
    "    print(f\"Training complete. Rewards saved to '{REWARDS_PATH}'.\")\n",
    "        \n",
    "else:\n",
    "    # Load the saved results from the training\n",
    "    if os.path.exists(REWARDS_PATH):\n",
    "        rewards_mc2_coin = np.load(REWARDS_PATH)\n",
    "        print(f\"Loaded rewards from '{REWARDS_PATH}'.\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No saved rewards found at '{REWARDS_PATH}'.\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(mc2_gen,pred_resp[:,-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"Times New Roman\",\n",
    "    \"font.size\": 19,\n",
    "    \"axes.titlesize\": 19,\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"legend.fontsize\": 12,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16\n",
    "})\n",
    "\n",
    "# ── Normalize and mask zero entries ─────────────────────────────────────\n",
    "data = rewards_mc2_coin\n",
    "data = (data - data.min()) / (data.max() - data.min())\n",
    "masked_data = np.ma.masked_where(data == 0, data)\n",
    "N, B = data.shape\n",
    "\n",
    "# ── Create modified colormap with black for masked values ───────────────\n",
    "cmap = plt.get_cmap('viridis').copy()\n",
    "cmap.set_bad(color='black')\n",
    "\n",
    "# ── Create figure and axes ──────────────────────────────────────────────\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# ── Plot heatmap ────────────────────────────────────────────────────────\n",
    "cax = ax.imshow(masked_data,\n",
    "                origin='upper',\n",
    "                aspect='auto',\n",
    "                interpolation='nearest',\n",
    "                vmin=0,\n",
    "                vmax=1,\n",
    "                cmap=cmap)\n",
    "\n",
    "# ── Highlight training data ─────────────────────────────────────────────\n",
    "d = mc_2[::500]\n",
    "col_indices = np.rint((d - mc2_gen.min()) / (mc2_gen.max() - mc2_gen.min()) * (mc2_gen.size - 1)).astype(int)\n",
    "row_indices = np.arange(N)\n",
    "\n",
    "ax.scatter(col_indices, row_indices,\n",
    "           facecolors='red',\n",
    "           edgecolors='white',\n",
    "           s=50,\n",
    "           linewidths=2,\n",
    "           marker='o')\n",
    "\n",
    "# ── Labels, ticks, colorbar ─────────────────────────────────────────\n",
    "fig.colorbar(cax, ax=ax, label='Return (Scaled)')\n",
    "ax.set_xlabel('Parameter Value')\n",
    "ax.set_ylabel('Episode')\n",
    "\n",
    "ax.set_xticks(np.arange(0, B, 2))\n",
    "ax.set_yticks(np.arange(0, N, 5))\n",
    "ax.set_xticklabels([f\"{float(v):.2f}\" for v in mc2_gen[::2]])\n",
    "ax.set_yticklabels(np.arange(0, len(mc_2), 2500))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/fig2a_coin_mc2.svg', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means: Mountain Car 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.clustering import kmeans_1d\n",
    "\n",
    "K = 4\n",
    "data = mc_2\n",
    "\n",
    "# Run k-means clustering\n",
    "labels_kmeans, centroids_kmeans = kmeans_1d(mc_2, K=K, verbose=False)\n",
    "\n",
    "# We use the COIN Q-learning framework, although with perfect context knowledge\n",
    "p_contexts_kmeans = np.zeros((data.shape[0],5))\n",
    "p_contexts_kmeans[labels_kmeans==0,0] = 1.0\n",
    "p_contexts_kmeans[labels_kmeans==1,1] = 1.0\n",
    "p_contexts_kmeans[labels_kmeans==2,2] = 1.0\n",
    "p_contexts_kmeans[labels_kmeans==3,3] = 1.0\n",
    "\n",
    "# Predictive responsibilities from k-means simply finds the closest context\n",
    "test_labels = np.argmin(np.abs(mc2_gen[:, None] - centroids_kmeans[None, :]), axis=1)\n",
    "\n",
    "pred_resp_k = np.zeros((test_labels.shape[0],5))\n",
    "pred_resp_k[test_labels==0,0] = 1.0\n",
    "pred_resp_k[test_labels==1,1] = 1.0\n",
    "pred_resp_k[test_labels==2,2] = 1.0\n",
    "pred_resp_k[test_labels==3,3] = 1.0\n",
    "\n",
    "pred_resp_k = pred_resp_k[:,None,:]\n",
    "pred_resp_k = np.repeat(pred_resp_k, data.shape[0], axis=1)\n",
    "\n",
    "REWARDS_PATH = 'models/fig2a_rewards_mc2_kmeans.npy'\n",
    "if train_mc2:\n",
    "    # Number of repetitions and number of parallel processes:\n",
    "    n_reps = 1\n",
    "    n_processes = 1\n",
    "        \n",
    "    # Run tests\n",
    "    all_results = run_single_rep(1, mc_2, p_contexts_kmeans, mc2_gen, pred_resp_k)\n",
    "\n",
    "    rewards_mc2_kmeans = all_results[0]\n",
    "\n",
    "    # Save results\n",
    "    np.save(REWARDS_PATH, rewards_mc2_kmeans)\n",
    "    print(f\"Training complete. Rewards saved to '{REWARDS_PATH}'.\")\n",
    "        \n",
    "else:\n",
    "    # Load the saved results from the training\n",
    "    if os.path.exists(REWARDS_PATH):\n",
    "        rewards_mc2_kmeans = np.load(REWARDS_PATH)\n",
    "        print(f\"Loaded rewards from '{REWARDS_PATH}'.\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No saved rewards found at '{REWARDS_PATH}'.\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"Times New Roman\",\n",
    "    \"font.size\": 19,\n",
    "    \"axes.titlesize\": 19,\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"legend.fontsize\": 12,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16\n",
    "})\n",
    "\n",
    "# ── Normalize and mask zero entries ─────────────────────────────────────\n",
    "data = rewards_mc2_kmeans\n",
    "data = (data - data.min()) / (data.max() - data.min())\n",
    "masked_data = np.ma.masked_where(data == 0, data)\n",
    "N, B = data.shape\n",
    "\n",
    "# ── Create colormap with black for masked (zero) values ────────────────\n",
    "cmap = plt.get_cmap('viridis').copy()\n",
    "cmap.set_bad(color='black')\n",
    "\n",
    "# ── Plot heatmap ───────────────────────────────────────────────────────\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "cax = ax.imshow(masked_data,\n",
    "                origin='upper',\n",
    "                aspect='auto',\n",
    "                interpolation='nearest',\n",
    "                vmin=0,\n",
    "                vmax=1,\n",
    "                cmap=cmap)\n",
    "\n",
    "# ── Highlight training data points ─────────────────────────────────────\n",
    "d = mc_2[::500]\n",
    "col_indices = np.rint((d - mc2_gen.min()) / (mc2_gen.max() - mc2_gen.min()) * (mc2_gen.size - 1)).astype(int)\n",
    "row_indices = np.arange(N)\n",
    "\n",
    "ax.scatter(col_indices, row_indices,\n",
    "           facecolors='red',\n",
    "           edgecolors='white',\n",
    "           s=50,\n",
    "           linewidths=2,\n",
    "           marker='o')\n",
    "\n",
    "# ── Labels, ticks, and colorbar ────────────────────────────────────────\n",
    "fig.colorbar(cax, ax=ax, label='Return (Scaled)')\n",
    "ax.set_xlabel('Parameter Value')\n",
    "ax.set_ylabel('Episode')\n",
    "\n",
    "ax.set_xticks(np.arange(0, B, 2))\n",
    "ax.set_yticks(np.arange(0, N, 5))\n",
    "ax.set_xticklabels([f\"{float(v):.2f}\" for v in mc2_gen[::2]])\n",
    "ax.set_yticklabels(np.arange(0, len(mc_2), 2500))\n",
    "\n",
    "# Optional rotation for crowded x-ticks:\n",
    "# plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/fig2a_kmeans_mc2.svg', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole: PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for cartpole. We try two different parameter variations, mass and length\n",
    "data = np.concatenate([\n",
    "    0.2*np.ones((5,)),\n",
    "    1.0*np.ones((5,)),\n",
    "    np.linspace(1.0, 1.8, 5),\n",
    "    np.ones((5,))\n",
    "])\n",
    "\n",
    "cartp_gen = np.linspace(0.2,1.8,21) # Data to test generalisation\n",
    "\n",
    "# Train or load\n",
    "train_cartp = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COIN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train COIN Model\n",
    "coin_model = COIN(\n",
    "    sigma_sensory_noise = 0.003, \n",
    "    sigma_motor_noise = 0.00182,\n",
    ")\n",
    "\n",
    "coin_model.perturbations = data # We now deal with epochs, which comprise rollouts of many possible episodes\n",
    "\n",
    "output = coin_model.simulate_coin()\n",
    "\n",
    "known_c_resp, novel_c_resp = coin_model.get_responsibilities(output)\n",
    "p_contexts = np.concatenate([known_c_resp,novel_c_resp[:,None]],axis=-1) \n",
    "\n",
    "# Ensure COIN probabilities are of the right form\n",
    "# Novel context - should never have a NaN definition\n",
    "p_temp = p_contexts.copy()\n",
    "p_temp[np.isnan(p_temp)] = 0.0\n",
    "p_contexts[:,-1] = 1 - np.sum(p_temp[:,:-1], axis=1) # This variable has size [T,C]. It should be used for TRAINING, passed into the p_context parameter\n",
    "\n",
    "\n",
    "pred_resp = coin_model.get_predicted_responsibilities(output, cartp_gen) #This has size [N,T,C] - for gen_val. Each evaluation takes a new value of size [N,C]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO COIN-RL Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_rep(data, p_context, gen_val, pred_resp, param_name='length'):\n",
    "    \"\"\"\n",
    "    Runs one repetition of training across all data points. After every training step, evaluate for every \n",
    "    amplitude in gen_val.\n",
    "    Returns the list of rewards for the evaluation steps.\n",
    "    \"\"\"\n",
    "    # Imports for multiprocessing\n",
    "    from environments import CustomCartPoleEnv\n",
    "    from rl import COINPPOAgent\n",
    "    import numpy as np\n",
    "    from tqdm.notebook import tqdm\n",
    "    # Create a fresh agent and environment inside each process\n",
    "\n",
    "    C = p_context.shape[1]-1\n",
    "\n",
    "    # Write data to context-prob dictionary form\n",
    "    data_dict = {}\n",
    "    J = p_context.shape[1]-1 # number of contexts (excluding novel)\n",
    "    for i in range(len(data)):\n",
    "        data_dict[i] = {}\n",
    "        for j in range(J):\n",
    "            data_dict[i][j+1] = p_context[i,j]\n",
    "        data_dict[i]['novel'] = p_context[i,-1]\n",
    "\n",
    "    # Also write pred_resp to dictionary form\n",
    "    pred_resp_dicts = []\n",
    "    for i in range(pred_resp.shape[0]):\n",
    "        pred_resp_dicts.append([])\n",
    "        for t in range(pred_resp.shape[1]):\n",
    "            pred_resp_dicts[i].append({})\n",
    "            for j in range(J):\n",
    "                pred_resp_dicts[i][t][j+1] = pred_resp[i,t,j]\n",
    "            pred_resp_dicts[i][t]['novel'] = pred_resp[i,t,-1]\n",
    "\n",
    "    kwargs = {param_name: 1.0}\n",
    "    env = CustomCartPoleEnv(render_mode=\"none\", **kwargs)\n",
    "    agent = COINPPOAgent(base_obs_dim=4, act_dim=2, ctx_ids=data_dict[0].keys())\n",
    "\n",
    "    rewards_for_this_rep = []\n",
    "    training_rewards_for_this_rep = []\n",
    "    evaluation_rewards_for_this_rep = []\n",
    "    for epoch, param_val in tqdm(enumerate(data), total=len(data)):\n",
    "        # Create the environment for each amplitude\n",
    "        kwargs = {param_name: param_val}\n",
    "        env = CustomCartPoleEnv(render_mode=\"none\", **kwargs)\n",
    "\n",
    "        # Set current context function\n",
    "        context_fcn = lambda i: data_dict[epoch]\n",
    "\n",
    "        r = agent.train_step(env, context_probs_fn=context_fcn) # Rollout steps here\n",
    "        mean_return = r[\"mean_episode_return\"]\n",
    "        training_rewards_for_this_rep.append(mean_return)\n",
    "\n",
    "        eval_r = agent.evaluate(env, context_probs_fn=context_fcn, n_episodes=1)[0]\n",
    "        evaluation_rewards_for_this_rep.append(eval_r)\n",
    "          \n",
    "        # Evaluate for all the gen_val\n",
    "        evaluation_rewards = np.zeros((gen_val.size,))\n",
    "\n",
    "        for i in range(evaluation_rewards.size):\n",
    "            kwargs = {param_name: gen_val[i]}\n",
    "            env = CustomCartPoleEnv(render_mode=\"none\", **kwargs)\n",
    "\n",
    "            context_fcn = lambda x: pred_resp_dicts[i][epoch]\n",
    "            evaluation_rewards[i] = np.mean(agent.evaluate(env, context_probs_fn=context_fcn, n_episodes=10, ignore_novel=True)[0])\n",
    "        rewards_for_this_rep.append(evaluation_rewards)\n",
    "\n",
    "    return np.stack(rewards_for_this_rep), np.stack(training_rewards_for_this_rep), np.stack(evaluation_rewards_for_this_rep)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REWARDS_PATH_LENGTH = 'models/fig2a_rewards_cartp_coin_length.npy'\n",
    "REWARDS_PATH_MASS = 'models/fig2a_rewards_cartp_coin_mass.npy'\n",
    "if train_cartp:\n",
    "    all_results_length = run_single_rep(data, p_contexts, cartp_gen, pred_resp, param_name='length')\n",
    "\n",
    "    rewards_cartp_coin_length = all_results_length[0]\n",
    "\n",
    "    # Save results\n",
    "    np.save(REWARDS_PATH_LENGTH, rewards_cartp_coin_length)\n",
    "    print(f\"Training complete. Rewards saved to '{REWARDS_PATH_LENGTH}'.\")\n",
    "\n",
    "    all_results_mass = run_single_rep(data, p_contexts, cartp_gen, pred_resp, param_name='masscart')\n",
    "\n",
    "    rewards_cartp_coin_mass = all_results_mass[0]\n",
    "\n",
    "    # Save results\n",
    "    np.save(REWARDS_PATH_MASS, rewards_cartp_coin_mass)\n",
    "    print(f\"Training complete. Rewards saved to '{REWARDS_PATH_MASS}'.\")\n",
    "        \n",
    "else:\n",
    "    # Load the saved results from the training\n",
    "    if os.path.exists(REWARDS_PATH_LENGTH):\n",
    "        rewards_cartp_coin_length = np.load(REWARDS_PATH_LENGTH)\n",
    "        print(f\"Loaded rewards from '{REWARDS_PATH_LENGTH}'.\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No saved rewards found at '{REWARDS_PATH_LENGTH}'.\")\n",
    "\n",
    "    if os.path.exists(REWARDS_PATH_MASS):\n",
    "        rewards_cartp_coin_mass = np.load(REWARDS_PATH_MASS)\n",
    "        print(f\"Loaded rewards from '{REWARDS_PATH_MASS}'.\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No saved rewards found at '{REWARDS_PATH_MASS}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"Times New Roman\",\n",
    "    \"font.size\": 19,\n",
    "    \"axes.titlesize\": 19,\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"legend.fontsize\": 12,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16\n",
    "})\n",
    "\n",
    "# ── Normalize and mask zero entries ─────────────────────────────────────\n",
    "r = rewards_cartp_coin_length\n",
    "r = (r - r.min()) / (r.max() - r.min())\n",
    "masked_r = np.ma.masked_where(r == 0, r)\n",
    "N, B = r.shape\n",
    "\n",
    "# ── Create colormap with black for masked (zero) values ────────────────\n",
    "cmap = plt.get_cmap('viridis').copy()\n",
    "cmap.set_bad(color='black')\n",
    "\n",
    "# ── Create figure and axes ─────────────────────────────────────────────\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "cax = ax.imshow(masked_r,\n",
    "                origin='upper',\n",
    "                aspect='auto',\n",
    "                interpolation='nearest',\n",
    "                vmin=0,\n",
    "                vmax=1,\n",
    "                cmap=cmap)\n",
    "\n",
    "# ── Highlight training data points ─────────────────────────────────────\n",
    "d = data\n",
    "col_indices = np.rint((d - cartp_gen.min()) / (cartp_gen.max() - cartp_gen.min()) * (cartp_gen.size - 1)).astype(int)\n",
    "row_indices = np.arange(N)\n",
    "\n",
    "ax.scatter(col_indices, row_indices,\n",
    "           facecolors='red',\n",
    "           edgecolors='white',\n",
    "           s=50,\n",
    "           linewidths=2,\n",
    "           marker='o')\n",
    "\n",
    "# ── Labels, ticks, and colorbar ────────────────────────────────────────\n",
    "fig.colorbar(cax, ax=ax, label='Return (Scaled)')\n",
    "ax.set_xlabel('Parameter Value')\n",
    "ax.set_ylabel('Epoch')\n",
    "\n",
    "ax.set_xticks(np.arange(0, B, 2))\n",
    "ax.set_yticks(np.arange(0, N, 5))\n",
    "ax.set_xticklabels([f\"{float(v):.2f}\" for v in cartp_gen[::2]])\n",
    "ax.set_yticklabels(np.arange(0, len(data), 5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/fig2a_coin_cartp_length.svg', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalisation during testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Data for testing\n",
    "data = np.concatenate([\n",
    "    0.2*np.ones((2, )),\n",
    "    np.linspace(0.2, 1.0, 5),\n",
    "    np.ones((2,)),\n",
    "    0.2*np.ones((3, )),\n",
    "    np.ones((2, )),\n",
    "    1.5*np.ones((5, )),\n",
    "])\n",
    "\n",
    "test_data = np.linspace(0.01, 2.0, 100)\n",
    "\n",
    "# Plot data as 1d histogram with density indicated by transparency\n",
    "# plt.figure(figsize=(8, 1))\n",
    "# plt.scatter(data, np.zeros_like(data), alpha=0.2, edgecolor='none', color='blue', s=100)\n",
    "# plt.yticks([])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COIN and K-Means Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train COIN model\n",
    "coin_model = COIN(\n",
    "    sigma_sensory_noise = 0.003, \n",
    "    sigma_motor_noise = 0.00182,\n",
    ")\n",
    "\n",
    "coin_model.perturbations = data # Using COIN-PPO\n",
    "\n",
    "output = coin_model.simulate_coin()\n",
    "\n",
    "known_c_resp, novel_c_resp = coin_model.get_responsibilities(output)\n",
    "p_contexts_coin = np.concatenate([known_c_resp,novel_c_resp[:,None]],axis=-1) \n",
    "\n",
    "# Ensure COIN probabilities are of the right form\n",
    "# Novel context - should never have a NaN definition\n",
    "p_temp = p_contexts_coin.copy()\n",
    "p_temp[np.isnan(p_temp)] = 0.0\n",
    "p_contexts_coin[:,-1] = 1 - np.sum(p_temp[:,:-1], axis=1) # This variable has size [T,C]. It should be used for TRAINING, passed into the p_context parameter\n",
    "\n",
    "\n",
    "pred_resp_coin = coin_model.get_predicted_responsibilities(output, test_data)[:,-1,:] # For testing, we only care about the last training result\n",
    "\n",
    "# K-Means\n",
    "from utils.clustering import kmeans_1d\n",
    "\n",
    "K = 4 # 'guess' number of contexts\n",
    "\n",
    "# Run k-means clustering\n",
    "labels_kmeans, centroids_kmeans = kmeans_1d(data, K=K, verbose=False)\n",
    "\n",
    "# We use the COIN Q-learning framework, although with perfect context knowledge\n",
    "p_contexts_kmeans = np.zeros((data.shape[0],5))\n",
    "p_contexts_kmeans[labels_kmeans==0,0] = 1.0\n",
    "p_contexts_kmeans[labels_kmeans==1,1] = 1.0\n",
    "p_contexts_kmeans[labels_kmeans==2,2] = 1.0\n",
    "p_contexts_kmeans[labels_kmeans==3,3] = 1.0\n",
    "\n",
    "# Predictive responsibilities from k-means simply finds the closest context\n",
    "test_labels = np.argmin(np.abs(test_data[:, None] - centroids_kmeans[None, :]), axis=1)\n",
    "\n",
    "pred_resp_k = np.zeros((test_labels.shape[0],5))\n",
    "pred_resp_k[test_labels==0,0] = 1.0\n",
    "pred_resp_k[test_labels==1,1] = 1.0\n",
    "pred_resp_k[test_labels==2,2] = 1.0\n",
    "pred_resp_k[test_labels==3,3] = 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(p_contexts_coin)\n",
    "plt.title('COIN Context Probabilities')\n",
    "plt.subplot(2, 2, 2)\n",
    "# Sorted unique labels (in case k-means returned [2, 0, 1] etc.)\n",
    "unique_labels = np.unique(labels_kmeans)\n",
    "K = len(unique_labels)\n",
    "cmap = plt.get_cmap('tab10')\n",
    "label_to_color = {label: cmap(i) for i, label in enumerate(unique_labels)}\n",
    "\n",
    "# Colors for each point in correct label order\n",
    "point_colors = [label_to_color[label] for label in labels_kmeans]\n",
    "\n",
    "# Create figure\n",
    "plt.scatter(\n",
    "    np.arange(data.size),\n",
    "    data,\n",
    "    c=point_colors,\n",
    "    s=60,\n",
    "    edgecolors='none'\n",
    ")\n",
    "\n",
    "plt.title('K-Means Context Assignments')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "# Remove novel probability if feasible - sum of other terms non-zero\n",
    "idx = np.where(np.nansum(pred_resp_coin[:,:-1], axis=1) > 0)[0]\n",
    "pred_resp_coin_new = pred_resp_coin.copy()\n",
    "pred_resp_coin_new[idx,-1] = 0.0\n",
    "pred_resp_coin_new[idx,:-1] = pred_resp_coin_new[idx,:-1] / np.nansum(pred_resp_coin_new[idx,:-1], axis=1)[:,None]\n",
    "for j in range(pred_resp_coin.shape[1]):\n",
    "    if j == pred_resp_coin.shape[1]-1:\n",
    "        plt.plot(test_data, pred_resp_coin_new[:,j], label='Novel Context', alpha=1.0)\n",
    "    else:\n",
    "        plt.plot(test_data, pred_resp_coin_new[:,j], label=f'Context {j+1}', alpha=1.0)\n",
    "plt.legend()\n",
    "\n",
    "plt.title('COIN Predictive Responsibilities')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "for j in range(pred_resp_k.shape[1]):\n",
    "    plt.plot(test_data, pred_resp_k[:,j], label=f'Context {j+1}', alpha=1.0)\n",
    "plt.legend()\n",
    "plt.title('K-Means Predictive Responsibilities')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import spaces\n",
    "def _get_action_dim(action_space):\n",
    "    if isinstance(action_space, spaces.Discrete):\n",
    "        return action_space.n                     # only one discrete action dimension\n",
    "    elif isinstance(action_space, spaces.Box):\n",
    "        return int(np.prod(action_space.shape))  # flatten the Box shape\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Unsupported action space type: {type(action_space)}\")\n",
    "\n",
    "def train_env(env, data, p_context, param_name):\n",
    "    \"\"\"\n",
    "    Train given environment on a COIN-PPO agent ready for testing. Context parameter name must be given.\n",
    "    Returns the trained agent, and some reward information.\n",
    "    \"\"\"\n",
    "    # Import\n",
    "    from rl import COINPPOAgent\n",
    "    import numpy as np\n",
    "    from tqdm.notebook import tqdm\n",
    "\n",
    "    # Write data to context-prob dictionary form\n",
    "    data_dict = {}\n",
    "    J = p_context.shape[1]-1 # number of contexts (excluding novel)\n",
    "    for i in range(len(data)):\n",
    "        data_dict[i] = {}\n",
    "        for j in range(J):\n",
    "            data_dict[i][j+1] = p_context[i,j]\n",
    "        data_dict[i]['novel'] = p_context[i,-1]\n",
    "\n",
    "\n",
    "    # Create the COIN agent\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = _get_action_dim(env.action_space)\n",
    "    agent = COINPPOAgent(base_obs_dim=obs_dim, act_dim=act_dim, ctx_ids=data_dict[0].keys(), action_continuous=isinstance(env.action_space, spaces.Box))\n",
    "\n",
    "    training_rewards_for_this_rep = []\n",
    "    evaluation_rewards_for_this_rep = []\n",
    "\n",
    "    # Wrap the enumerate in a tqdm iterator\n",
    "    pbar = tqdm(enumerate(data), total=len(data), desc=\"Epoch\")\n",
    "\n",
    "    for epoch, param_val in pbar:\n",
    "        # Set context for the environment in this epoch\n",
    "        setattr(env, param_name, param_val)\n",
    "        env.reset()\n",
    "\n",
    "        # Set current context function\n",
    "        context_fcn = lambda i: data_dict[epoch]\n",
    "\n",
    "        r = agent.train_step(env, context_probs_fn=context_fcn) # Rollout steps here\n",
    "        mean_return = r[\"mean_episode_return\"]\n",
    "        training_rewards_for_this_rep.append(mean_return)\n",
    "\n",
    "        eval_r = agent.evaluate(env, context_probs_fn=context_fcn, n_episodes=1)[0]\n",
    "        evaluation_rewards_for_this_rep.append(eval_r)\n",
    "\n",
    "        # Update the progress bar with both training & eval rewards\n",
    "        pbar.set_postfix({\n",
    "            \"train\": f\"{mean_return:.2f}\",\n",
    "            \"eval\":  f\"{eval_r:.2f}\"\n",
    "        })\n",
    "\n",
    "    return agent, np.stack(training_rewards_for_this_rep), np.stack(evaluation_rewards_for_this_rep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CartPole and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── imports ─────────────────────────────────────────────────────────────────\n",
    "import os, joblib\n",
    "from environments import CustomCartPoleEnv\n",
    "\n",
    "# ── constants ───────────────────────────────────────────────────────────────\n",
    "TRAIN_2B_CARTPOLE = False\n",
    "PATH_PREFIX       = \"models/fig2b_cartpole_\"\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "PARAM_SETTINGS = {\n",
    "    \"length\"  : dict(data=data,           tag=\"length\"),\n",
    "    \"masspole\": dict(data=data,           tag=\"mass\"),\n",
    "    \"gravity\" : dict(data=data * 9.8,     tag=\"gravity\"),\n",
    "}\n",
    "\n",
    "ALGORITHMS = {\n",
    "    \"coin\"  : p_contexts_coin,\n",
    "    \"kmeans\": p_contexts_kmeans,\n",
    "}\n",
    "\n",
    "# ── helpers ─────────────────────────────────────────────────────────────────\n",
    "def save_agent(obj, path): joblib.dump(obj, path, compress=3)\n",
    "def load_agent(path):      return joblib.load(path)\n",
    "\n",
    "# ── central registries ──────────────────────────────────────────────────────\n",
    "all_agents_cpole   = {}   # key → agent\n",
    "all_train_rewards  = {}   # key → np.ndarray  (per-epoch training return)\n",
    "all_eval_rewards   = {}   # key → np.ndarray  (per-epoch evaluation return)\n",
    "\n",
    "# ── train or load ───────────────────────────────────────────────────────────\n",
    "if TRAIN_2B_CARTPOLE:\n",
    "    env = CustomCartPoleEnv(render_mode=\"none\")\n",
    "\n",
    "    for algo_name, p_ctx in ALGORITHMS.items():\n",
    "        for attr_name, cfg in PARAM_SETTINGS.items():\n",
    "            print(f\"\\n▶ Training {algo_name.upper()} on '{attr_name}' …\")\n",
    "\n",
    "            agent, train_rs, eval_rs = train_env(\n",
    "                env, cfg[\"data\"], p_ctx, attr_name\n",
    "            )\n",
    "\n",
    "            key = f\"{algo_name}_{cfg['tag']}\"\n",
    "            save_agent(agent, f\"{PATH_PREFIX}{key}.pkl\")\n",
    "\n",
    "            # -- stash everything in RAM for quick inspection --\n",
    "            all_agents_cpole[key]  = agent\n",
    "            all_train_rewards[key] = train_rs\n",
    "            all_eval_rewards[key]  = eval_rs\n",
    "\n",
    "            print(f\"✓ Saved & registered as '{key}'\")\n",
    "\n",
    "    print(f\"\\nStored {len(all_agents_cpole)} agents in memory.\")\n",
    "\n",
    "else:\n",
    "    # load agents; rewards stay empty (weren’t saved)\n",
    "    all_agents_cpole = {\n",
    "        f\"{algo}_{cfg['tag']}\":\n",
    "            load_agent(f\"{PATH_PREFIX}{algo}_{cfg['tag']}.pkl\")\n",
    "        for algo in ALGORITHMS\n",
    "        for cfg  in PARAM_SETTINGS.values()\n",
    "    }\n",
    "    print(f\"Loaded {len(all_agents_cpole)} trained agents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "TEST_2B_CARTPOLE = False\n",
    "\n",
    "# ── predicted responsibility dictionaries ───────────────────────────────\n",
    "RESP_DICTS = {}\n",
    "RESP_MATS = {\n",
    "    \"coin\"  : pred_resp_coin,\n",
    "    \"kmeans\": pred_resp_k,\n",
    "}\n",
    "for alg, mat in RESP_MATS.items():\n",
    "    data_dict = {}\n",
    "    J = mat.shape[1]-1 # number of contexts (excluding novel)\n",
    "    for i in range(mat.shape[0]):\n",
    "        data_dict[i] = {}\n",
    "        for j in range(J):\n",
    "            data_dict[i][j+1] = mat[i,j]\n",
    "        data_dict[i]['novel'] = mat[i,-1]\n",
    "    RESP_DICTS[alg] = data_dict\n",
    "\n",
    "\n",
    "def make_test_env(attr_name: str, value: float):\n",
    "    return CustomCartPoleEnv(**{attr_name: value, \"render_mode\": \"none\"})\n",
    "\n",
    "generalisation_rewards = {}      # key → np.ndarray[len(test_data)]\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "if TEST_2B_CARTPOLE:\n",
    "\n",
    "    outer = tqdm(all_agents_cpole.items(),\n",
    "                 total=len(all_agents_cpole),\n",
    "                 desc=\"Agents\",\n",
    "                 leave=False)\n",
    "\n",
    "    for key, agent in outer:\n",
    "        algo, tag = key.split(\"_\", 1)\n",
    "\n",
    "        attr_name = {\"length\": \"length\",\n",
    "                     \"mass\"  : \"masspole\",\n",
    "                     \"gravity\": \"gravity\"}[tag]\n",
    "\n",
    "        resp_dict = RESP_DICTS[algo]\n",
    "        rewards  = np.empty(len(test_data))\n",
    "\n",
    "        inner = tqdm(enumerate(test_data),\n",
    "                     total=len(test_data),\n",
    "                     desc=f\"{key:>14s}\",\n",
    "                     leave=False,\n",
    "                     position=1)\n",
    "\n",
    "        for i, v in inner:\n",
    "            value = v * 9.8 if tag == \"gravity\" else v\n",
    "            env   = make_test_env(attr_name, value)\n",
    "            context_fcn = lambda _ : resp_dict[i]\n",
    "\n",
    "            rewards[i] = np.mean(\n",
    "                agent.evaluate(env,\n",
    "                               context_probs_fn=context_fcn,\n",
    "                               n_episodes=10,\n",
    "                               ignore_novel=True)\n",
    "            )\n",
    "\n",
    "            # show live average so far\n",
    "            inner.set_postfix(avg=f\"{rewards[:i+1].mean():.2f}\")\n",
    "\n",
    "        generalisation_rewards[key] = rewards\n",
    "        np.save(f\"{PATH_PREFIX}test_{key}.npy\", rewards)\n",
    "        outer.set_postfix(last=f\"{rewards.mean():.2f}\")\n",
    "\n",
    "    print(\"\\n✓ Generalisation rewards saved for\",\n",
    "          f\"{len(generalisation_rewards)} agents.\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "else:\n",
    "    # pretty progress while reloading from disk\n",
    "    path_iter = (\n",
    "        (f\"{algo}_{cfg['tag']}\", f\"{PATH_PREFIX}test_{algo}_{cfg['tag']}.npy\")\n",
    "        for algo in ALGORITHMS for cfg in PARAM_SETTINGS.values()\n",
    "    )\n",
    "\n",
    "    for key, fpath in tqdm(list(path_iter),\n",
    "                           desc=\"Loading reward arrays\",\n",
    "                           leave=False):\n",
    "        generalisation_rewards[key] = np.load(fpath)\n",
    "\n",
    "    print(f\"Reloaded {len(generalisation_rewards)} generalisation arrays:\")\n",
    "    for k, arr in generalisation_rewards.items():\n",
    "        print(f\" • {k:15s}  shape={arr.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    \"font.family\": \"Times New Roman\",\n",
    "    \"font.size\": 19,\n",
    "    \"axes.titlesize\": 19,\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"legend.fontsize\": 12,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16\n",
    "})\n",
    "\n",
    "# ── Smooth the generalisation reward curves ─────────────────────────\n",
    "def smoothen(data, window_size=5):\n",
    "    \"\"\"Smoothen the data with a spatial average.\"\"\"\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='same')\n",
    "\n",
    "smoothened_rewards = {\n",
    "    key: smoothen(val, window_size=50)\n",
    "    for key, val in generalisation_rewards.items()\n",
    "}\n",
    "\n",
    "# ── Color scheme and figure setup ───────────────────────────────────\n",
    "col = {\"length\": \"blue\", \"mass\": \"orange\", \"gravity\": \"green\"}\n",
    "fig, axs = plt.subplots(3, 1, figsize=(10, 17), sharex=True)\n",
    "\n",
    "# ── Plot per parameter ──────────────────────────────────────────────\n",
    "for idx, param in enumerate([\"length\", \"mass\", \"gravity\"]):\n",
    "    ax = axs[idx]\n",
    "\n",
    "    for key, val in smoothened_rewards.items():\n",
    "        algo, tag = key.split(\"_\", 1)\n",
    "        if tag != param:\n",
    "            continue\n",
    "\n",
    "        linestyle = '-' if algo == \"coin\" else '--'\n",
    "        label = f\"{algo.upper()}\"\n",
    "        ax.plot(test_data, val, label=label, color=col[param], linestyle=linestyle, linewidth=2)\n",
    "\n",
    "    # Optionally highlight the training data positions\n",
    "    ax.scatter(data, np.zeros_like(data), alpha=0.15, edgecolor='none',\n",
    "               color='red', s=60, label=\"Train data\")\n",
    "\n",
    "    ax.set_title(f\"{param.capitalize()}\" if param != \"mass\" else \"MassPole\")\n",
    "    ax.set_ylabel(\"Smoothed Return\")\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.legend()\n",
    "\n",
    "axs[-1].set_xlabel(\"Test Parameter Value\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/fig2b_cartpole.svg', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extras - Not in paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Pendulum and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── imports ─────────────────────────────────────────────────────────────────\n",
    "import os, joblib\n",
    "from environments import CustomPendulumEnv\n",
    "\n",
    "# ── constants ───────────────────────────────────────────────────────────────\n",
    "TRAIN_2B_PEND = True\n",
    "PATH_PREFIX       = \"models/fig2b_pendulum_\"\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "PARAM_SETTINGS = {\n",
    "    \"l\"  : dict(data=data,           tag=\"l\"),\n",
    "    \"m\": dict(data=data,           tag=\"m\"),\n",
    "    \"g\" : dict(data=data * 9.8,     tag=\"g\"),\n",
    "}\n",
    "\n",
    "ALGORITHMS = {\n",
    "    \"coin\"  : p_contexts_coin,\n",
    "    \"kmeans\": p_contexts_kmeans,\n",
    "}\n",
    "\n",
    "# ── helpers ─────────────────────────────────────────────────────────────────\n",
    "def save_agent(obj, path): joblib.dump(obj, path, compress=3)\n",
    "def load_agent(path):      return joblib.load(path)\n",
    "\n",
    "# ── central registries ──────────────────────────────────────────────────────\n",
    "all_agents_pend   = {}   # key → agent\n",
    "all_train_rewards  = {}   # key → np.ndarray  (per-epoch training return)\n",
    "all_eval_rewards   = {}   # key → np.ndarray  (per-epoch evaluation return)\n",
    "\n",
    "# ── train or load ───────────────────────────────────────────────────────────\n",
    "if TRAIN_2B_PEND:\n",
    "    env = CustomPendulumEnv(render_mode=\"none\")\n",
    "\n",
    "    for algo_name, p_ctx in ALGORITHMS.items():\n",
    "        for attr_name, cfg in PARAM_SETTINGS.items():\n",
    "            print(f\"\\n▶ Training {algo_name.upper()} on '{attr_name}' …\")\n",
    "\n",
    "            agent, train_rs, eval_rs = train_env(\n",
    "                env, cfg[\"data\"], p_ctx, attr_name\n",
    "            )\n",
    "\n",
    "            key = f\"{algo_name}_{cfg['tag']}\"\n",
    "            save_agent(agent, f\"{PATH_PREFIX}{key}.pkl\")\n",
    "\n",
    "            # -- stash everything in RAM for quick inspection --\n",
    "            all_agents_pend[key]  = agent\n",
    "            all_train_rewards[key] = train_rs\n",
    "            all_eval_rewards[key]  = eval_rs\n",
    "\n",
    "            print(f\"✓ Saved & registered as '{key}'\")\n",
    "\n",
    "    print(f\"\\nStored {len(all_agents_pend)} agents in memory.\")\n",
    "\n",
    "else:\n",
    "    # load agents; rewards stay empty (weren’t saved)\n",
    "    all_agents_pend = {\n",
    "        f\"{algo}_{cfg['tag']}\":\n",
    "            load_agent(f\"{PATH_PREFIX}{algo}_{cfg['tag']}.pkl\")\n",
    "        for algo in ALGORITHMS\n",
    "        for cfg  in PARAM_SETTINGS.values()\n",
    "    }\n",
    "    print(f\"Loaded {len(all_agents_pend)} trained agents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "TEST_2B_PEND = True\n",
    "\n",
    "# ── predicted responsibility dictionaries ───────────────────────────────\n",
    "RESP_DICTS = {}\n",
    "RESP_MATS = {\n",
    "    \"coin\"  : pred_resp_coin,\n",
    "    \"kmeans\": pred_resp_k,\n",
    "}\n",
    "for alg, mat in RESP_MATS.items():\n",
    "    data_dict = {}\n",
    "    J = mat.shape[1]-1 # number of contexts (excluding novel)\n",
    "    for i in range(mat.shape[0]):\n",
    "        data_dict[i] = {}\n",
    "        for j in range(J):\n",
    "            data_dict[i][j+1] = mat[i,j]\n",
    "        data_dict[i]['novel'] = mat[i,-1]\n",
    "    RESP_DICTS[alg] = data_dict\n",
    "\n",
    "\n",
    "def make_test_env(attr_name: str, value: float):\n",
    "    return CustomPendulumEnv(**{attr_name: value, \"render_mode\": \"none\"})\n",
    "\n",
    "generalisation_rewards = {}      # key → np.ndarray[len(test_data)]\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "if TEST_2B_PEND:\n",
    "\n",
    "    outer = tqdm(all_agents_pend.items(),\n",
    "                 total=len(all_agents_pend),\n",
    "                 desc=\"Agents\",\n",
    "                 leave=False)\n",
    "\n",
    "    for key, agent in outer:\n",
    "        algo, tag = key.split(\"_\", 1)\n",
    "\n",
    "        attr_name = {\"l\": \"l\",\n",
    "                     \"m\"  : \"m\",\n",
    "                     \"g\": \"g\"}[tag]\n",
    "\n",
    "        resp_dict = RESP_DICTS[algo]\n",
    "        rewards  = np.empty(len(test_data))\n",
    "\n",
    "        inner = tqdm(enumerate(test_data),\n",
    "                     total=len(test_data),\n",
    "                     desc=f\"{key:>14s}\",\n",
    "                     leave=False,\n",
    "                     position=1)\n",
    "\n",
    "        for i, v in inner:\n",
    "            value = v * 9.8 if tag == \"g\" else v\n",
    "            env   = make_test_env(attr_name, value)\n",
    "            context_fcn = lambda _ : resp_dict[i]\n",
    "\n",
    "            rewards[i] = np.mean(\n",
    "                agent.evaluate(env,\n",
    "                               context_probs_fn=context_fcn,\n",
    "                               n_episodes=10,\n",
    "                               ignore_novel=True)\n",
    "            )\n",
    "\n",
    "            # show live average so far\n",
    "            inner.set_postfix(avg=f\"{rewards[:i+1].mean():.2f}\")\n",
    "\n",
    "        generalisation_rewards[key] = rewards\n",
    "        np.save(f\"{PATH_PREFIX}test_{key}.npy\", rewards)\n",
    "        outer.set_postfix(last=f\"{rewards.mean():.2f}\")\n",
    "\n",
    "    print(\"\\n✓ Generalisation rewards saved for\",\n",
    "          f\"{len(generalisation_rewards)} agents.\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "else:\n",
    "    # pretty progress while reloading from disk\n",
    "    path_iter = (\n",
    "        (f\"{algo}_{cfg['tag']}\", f\"{PATH_PREFIX}gen_{algo}_{cfg['tag']}.npy\")\n",
    "        for algo in ALGORITHMS for cfg in PARAM_SETTINGS.values()\n",
    "    )\n",
    "\n",
    "    for key, fpath in tqdm(list(path_iter),\n",
    "                           desc=\"Loading reward arrays\",\n",
    "                           leave=False):\n",
    "        generalisation_rewards[key] = np.load(fpath)\n",
    "\n",
    "    print(f\"Reloaded {len(generalisation_rewards)} generalisation arrays:\")\n",
    "    for k, arr in generalisation_rewards.items():\n",
    "        print(f\" • {k:15s}  shape={arr.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoothen the data with simple moving average\n",
    "def smoothen(data, window_size=5):\n",
    "    \"\"\"Smoothen the data with a spatial average.\"\"\"\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='same')\n",
    "\n",
    "smoothened_rewards = {}\n",
    "for key, val in generalisation_rewards.items():\n",
    "    smoothened_rewards[key] = smoothen(val, window_size=50)\n",
    "\n",
    "# Plot all generalisation_rewards\n",
    "col = {\"l\": \"blue\",\n",
    "       \"m\"  : \"orange\",\n",
    "       \"g\": \"green\"}\n",
    "plt.figure(figsize=(8,10))\n",
    "for line, val in smoothened_rewards.items():\n",
    "    c = col[line.split(\"_\")[1]]\n",
    "    if line.startswith(\"coin\"):\n",
    "        # Use full line for coin\n",
    "        plt.plot(test_data, val, label=line, color=c)\n",
    "    else:\n",
    "        # Use dashed for k-means\n",
    "        plt.plot(test_data, val, label=line, color=c, linestyle='--')\n",
    "\n",
    "# Add simple scatter to demonstrate test_data\n",
    "plt.scatter(data, np.zeros_like(data), alpha=0.2, edgecolor='none', color='red', s=100)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Acrobot and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── imports ─────────────────────────────────────────────────────────────────\n",
    "import os, joblib\n",
    "from environments import CustomAcrobotEnv\n",
    "\n",
    "# ── constants ───────────────────────────────────────────────────────────────\n",
    "TRAIN_2B_ACRO = True\n",
    "PATH_PREFIX       = \"models/fig2b_acrobot_\"\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "PARAM_SETTINGS = {\n",
    "    \"link_length_1\"  : dict(data=data,           tag=\"link_length_1\"),\n",
    "    \"link_length_2\": dict(data=data,           tag=\"link_length_2\"),\n",
    "    \"gravity\" : dict(data=data * 9.8,     tag=\"gravity\"),\n",
    "}\n",
    "\n",
    "ALGORITHMS = {\n",
    "    \"coin\"  : p_contexts_coin,\n",
    "    \"kmeans\": p_contexts_kmeans,\n",
    "}\n",
    "\n",
    "# ── helpers ─────────────────────────────────────────────────────────────────\n",
    "def save_agent(obj, path): joblib.dump(obj, path, compress=3)\n",
    "def load_agent(path):      return joblib.load(path)\n",
    "\n",
    "# ── central registries ──────────────────────────────────────────────────────\n",
    "all_agents_acro   = {}   # key → agent\n",
    "all_train_rewards  = {}   # key → np.ndarray  (per-epoch training return)\n",
    "all_eval_rewards   = {}   # key → np.ndarray  (per-epoch evaluation return)\n",
    "\n",
    "# ── train or load ───────────────────────────────────────────────────────────\n",
    "if TRAIN_2B_ACRO:\n",
    "    env = CustomAcrobotEnv(render_mode=\"none\")\n",
    "\n",
    "    for algo_name, p_ctx in ALGORITHMS.items():\n",
    "        for attr_name, cfg in PARAM_SETTINGS.items():\n",
    "            print(f\"\\n▶ Training {algo_name.upper()} on '{attr_name}' …\")\n",
    "\n",
    "            agent, train_rs, eval_rs = train_env(\n",
    "                env, cfg[\"data\"], p_ctx, attr_name\n",
    "            )\n",
    "\n",
    "            key = f\"{algo_name}_{cfg['tag']}\"\n",
    "            save_agent(agent, f\"{PATH_PREFIX}{key}.pkl\")\n",
    "\n",
    "            # -- stash everything in RAM for quick inspection --\n",
    "            all_agents_acro[key]  = agent\n",
    "            all_train_rewards[key] = train_rs\n",
    "            all_eval_rewards[key]  = eval_rs\n",
    "\n",
    "            print(f\"✓ Saved & registered as '{key}'\")\n",
    "\n",
    "    print(f\"\\nStored {len(all_agents_acro)} agents in memory.\")\n",
    "\n",
    "else:\n",
    "    # load agents; rewards stay empty (weren’t saved)\n",
    "    all_agents_acro = {\n",
    "        f\"{algo}_{cfg['tag']}\":\n",
    "            load_agent(f\"{PATH_PREFIX}{algo}_{cfg['tag']}.pkl\")\n",
    "        for algo in ALGORITHMS\n",
    "        for cfg  in PARAM_SETTINGS.values()\n",
    "    }\n",
    "    print(f\"Loaded {len(all_agents_acro)} trained agents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "TEST_2B_ACRO = True\n",
    "\n",
    "# ── predicted responsibility dictionaries ───────────────────────────────\n",
    "RESP_DICTS = {}\n",
    "RESP_MATS = {\n",
    "    \"coin\"  : pred_resp_coin,\n",
    "    \"kmeans\": pred_resp_k,\n",
    "}\n",
    "for alg, mat in RESP_MATS.items():\n",
    "    data_dict = {}\n",
    "    J = mat.shape[1]-1 # number of contexts (excluding novel)\n",
    "    for i in range(mat.shape[0]):\n",
    "        data_dict[i] = {}\n",
    "        for j in range(J):\n",
    "            data_dict[i][j+1] = mat[i,j]\n",
    "        data_dict[i]['novel'] = mat[i,-1]\n",
    "    RESP_DICTS[alg] = data_dict\n",
    "\n",
    "\n",
    "def make_test_env(attr_name: str, value: float):\n",
    "    return CustomAcrobotEnv(**{attr_name: value, \"render_mode\": \"none\"})\n",
    "\n",
    "generalisation_rewards = {}      # key → np.ndarray[len(test_data)]\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "if TEST_2B_ACRO:\n",
    "\n",
    "    outer = tqdm(all_agents_acro.items(),\n",
    "                 total=len(all_agents_acro),\n",
    "                 desc=\"Agents\",\n",
    "                 leave=False)\n",
    "\n",
    "    for key, agent in outer:\n",
    "        algo, tag = key.split(\"_\", 1)\n",
    "\n",
    "        attr_name = {\"link_length_1\": \"link_length_1\",\n",
    "                     \"link_length_2\"  : \"link_length_2\",\n",
    "                     \"gravity\": \"gravity\"}[tag]\n",
    "\n",
    "        resp_dict = RESP_DICTS[algo]\n",
    "        rewards  = np.empty(len(test_data))\n",
    "\n",
    "        inner = tqdm(enumerate(test_data),\n",
    "                     total=len(test_data),\n",
    "                     desc=f\"{key:>14s}\",\n",
    "                     leave=False,\n",
    "                     position=1)\n",
    "\n",
    "        for i, v in inner:\n",
    "            value = v * 9.8 if tag == \"gravity\" else v\n",
    "            env   = make_test_env(attr_name, value)\n",
    "            context_fcn = lambda _ : resp_dict[i]\n",
    "\n",
    "            rewards[i] = np.mean(\n",
    "                agent.evaluate(env,\n",
    "                               context_probs_fn=context_fcn,\n",
    "                               n_episodes=10,\n",
    "                               ignore_novel=True)\n",
    "            )\n",
    "\n",
    "            # show live average so far\n",
    "            inner.set_postfix(avg=f\"{rewards[:i+1].mean():.2f}\")\n",
    "\n",
    "        generalisation_rewards[key] = rewards\n",
    "        np.save(f\"{PATH_PREFIX}test_{key}.npy\", rewards)\n",
    "        outer.set_postfix(last=f\"{rewards.mean():.2f}\")\n",
    "\n",
    "    print(\"\\n✓ Generalisation rewards saved for\",\n",
    "          f\"{len(generalisation_rewards)} agents.\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "else:\n",
    "    # pretty progress while reloading from disk\n",
    "    path_iter = (\n",
    "        (f\"{algo}_{cfg['tag']}\", f\"{PATH_PREFIX}gen_{algo}_{cfg['tag']}.npy\")\n",
    "        for algo in ALGORITHMS for cfg in PARAM_SETTINGS.values()\n",
    "    )\n",
    "\n",
    "    for key, fpath in tqdm(list(path_iter),\n",
    "                           desc=\"Loading reward arrays\",\n",
    "                           leave=False):\n",
    "        generalisation_rewards[key] = np.load(fpath)\n",
    "\n",
    "    print(f\"Reloaded {len(generalisation_rewards)} generalisation arrays:\")\n",
    "    for k, arr in generalisation_rewards.items():\n",
    "        print(f\" • {k:15s}  shape={arr.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoothen the data with simple moving average\n",
    "def smoothen(data, window_size=5):\n",
    "    \"\"\"Smoothen the data with a spatial average.\"\"\"\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='same')\n",
    "\n",
    "smoothened_rewards = {}\n",
    "for key, val in generalisation_rewards.items():\n",
    "    smoothened_rewards[key] = smoothen(val, window_size=50)\n",
    "\n",
    "# Plot all generalisation_rewards\n",
    "col = {\"gravity\": \"blue\",\n",
    "       \"link_length_1\"  : \"orange\",\n",
    "       \"link_length_2\": \"green\"}\n",
    "plt.figure(figsize=(8,10))\n",
    "for line, val in smoothened_rewards.items():\n",
    "    c = col[\"_\".join(line.split(\"_\")[1:])]\n",
    "    if line.startswith(\"coin\"):\n",
    "        # Use full line for coin\n",
    "        plt.plot(test_data, val, label=line, color=c)\n",
    "    else:\n",
    "        # Use dashed for k-means\n",
    "        plt.plot(test_data, val, label=line, color=c, linestyle='--')\n",
    "\n",
    "# Add simple scatter to demonstrate test_data\n",
    "plt.scatter(data, np.zeros_like(data), alpha=0.2, edgecolor='none', color='red', s=100)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 3: Continual Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Danger of Novel Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loss of generalisation - new novel context\n",
    "\n",
    "data = np.concatenate([\n",
    "    np.ones((5, )),\n",
    "    0.6*np.ones((5, )),\n",
    "]) # First data for Mountain Car\n",
    "\n",
    "timescale = 1000 # Number of \"episodes\" per \"trial\"\n",
    "data = np.repeat(data, timescale, axis=0)\n",
    "\n",
    "data_gen = np.linspace(0.0,2.0,21) # Data to test generalisation\n",
    "\n",
    "# Train or load\n",
    "train_3a = False\n",
    "\n",
    "# Train COIN Model\n",
    "coin_model = COIN(\n",
    "    sigma_sensory_noise = 0.003, \n",
    "    sigma_motor_noise = 0.00182,\n",
    "    prior_mean_retention= 0.99,\n",
    ")\n",
    "\n",
    "coin_model.perturbations = data[::timescale]\n",
    "\n",
    "output = coin_model.simulate_coin()\n",
    "\n",
    "known_c_resp, novel_c_resp = coin_model.get_responsibilities(output)\n",
    "p_contexts = np.concatenate([known_c_resp,novel_c_resp[:,None]],axis=-1) \n",
    "\n",
    "# Ensure COIN probabilities are of the right form\n",
    "# Novel context - should never have a NaN definition\n",
    "p_temp = p_contexts.copy()\n",
    "p_temp[np.isnan(p_temp)] = 0.0\n",
    "p_contexts[:,-1] = 1 - np.sum(p_temp[:,:-1], axis=1) # This variable has size [T,C]. It should be used for TRAINING, passed into the p_context parameter\n",
    "\n",
    "\n",
    "pred_resp = coin_model.get_predicted_responsibilities(output, data_gen) #This has size [N,T,C] - for gen_val. Each evaluation takes a new value of size [N,C].\n",
    "\n",
    "# Plotting - Not in Paper\n",
    "# Ensure interpolation of probabilities back to appropriate size\n",
    "p_contexts = np.repeat(p_contexts, repeats=timescale, axis=0)\n",
    "pred_resp = np.repeat(pred_resp, repeats=timescale, axis=1)\n",
    "plt.figure(figsize=(8, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(p_contexts)\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(data)\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(data_gen, pred_resp[:,-1,:])\n",
    "plt.subplot(2,2,4)\n",
    "\n",
    "# Remove novel probability if feasible - sum of other terms non-zero\n",
    "idx = np.where(np.nansum(pred_resp[:,-1,:-1], axis=1) > 0)[0]\n",
    "pred_resp_new = pred_resp[:,-1,:]\n",
    "pred_resp_new[idx,-1] = 0.0\n",
    "pred_resp_new[idx,:-1] = pred_resp_new[idx,:-1] / np.nansum(pred_resp_new[idx,:-1], axis=1)[:,None]\n",
    "plt.plot(data_gen, pred_resp_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plot_utils import plot_context_probabilities, plot_hor_context_prob\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"Times New Roman\",\n",
    "    \"font.size\": 19,\n",
    "    \"axes.titlesize\": 19,\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"legend.fontsize\": 12,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16\n",
    "})\n",
    "\n",
    "ax1 = plot_hor_context_prob(\n",
    "    pred_resp,\n",
    "    data_gen,\n",
    "    data,\n",
    "    1000,\n",
    "    min_intensity = 0.3,\n",
    "    fig_size=(12.5,3.75),\n",
    "    legend=False\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Set ax1 label\n",
    "ax1.set_xlabel(\"Episode\")\n",
    "ax1.set_ylabel(\"Contextual Probability\")\n",
    "\n",
    "plt.savefig('figures/fig3a_responsibilities.svg', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Remove novel probability if feasible - sum of other terms non-zero\n",
    "pr = pred_resp.copy()\n",
    "mask = np.nansum(pr[:, :, :-1], axis=2) > 0\n",
    "pr[:, :, -1] = np.where(mask, 0.0, pr[:, :, -1])\n",
    "known = pr[:, :, :-1]\n",
    "sum_known = np.nansum(known, axis=2, keepdims=True) \n",
    "renorm = np.where(mask[:, :, None],\n",
    "                  known / sum_known ,\n",
    "                  known)\n",
    "pr[:, :, :-1] = renorm\n",
    "\n",
    "# Plot novel-removed probabilities\n",
    "ax2 = plot_context_probabilities(\n",
    "    pr,\n",
    "    data_gen,\n",
    "    data,\n",
    "    1000,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find generalisation - as in Fig 2\n",
    "def run_single_rep(args):\n",
    "    \"\"\"\n",
    "    Runs one repetition of training across all amplitudes in sf. After every training step, evaluate for every \n",
    "    amplitude in gen_val.\n",
    "    Returns the list of rewards for the evaluation steps.\n",
    "    \"\"\"\n",
    "    sf, p_context, new_opt, test_point = args\n",
    "    # Imports for multiprocessing\n",
    "    from environments import CustomMountainCarEnv\n",
    "    from rl import COINQLearningAgent\n",
    "    import numpy as np\n",
    "    from tqdm.notebook import tqdm\n",
    "    # Create a fresh agent and environment inside each process\n",
    "\n",
    "    C = p_context.shape[1]-1\n",
    "\n",
    "    env = CustomMountainCarEnv(amplitude=1.0, render_mode=\"none\")\n",
    "    agent = COINQLearningAgent(\n",
    "        env=env,\n",
    "        max_contexts=C,\n",
    "        num_position_bins=30,\n",
    "        num_velocity_bins=30,\n",
    "        alpha=0.1,\n",
    "        gamma=0.99,\n",
    "        epsilon=1.0,\n",
    "        epsilon_decay=0.999,\n",
    "        instantiate_from_average=new_opt,\n",
    "        avoid_novel=new_opt,\n",
    "    )\n",
    "\n",
    "    training_rewards_for_this_rep = []\n",
    "    evaluation_rewards_for_this_rep = []\n",
    "    for t, amplitude in tqdm(enumerate(sf), total=len(sf)):\n",
    "        # Create the environment for each amplitude\n",
    "        env = CustomMountainCarEnv(amplitude=amplitude, render_mode=\"none\")\n",
    "\n",
    "        # If the test point is reached, set max epsilon of agent to 0.3 - simulate testing from then on\n",
    "        if test_point is not None and t >= test_point:\n",
    "            agent.max_epsilon = 0.3\n",
    "        \n",
    "        # Train the agent in the current context\n",
    "        training_reward = agent.train_step(env=env, p_context=p_context[t,:], max_steps_per_episode=200)\n",
    "        training_rewards_for_this_rep.append(training_reward)\n",
    "\n",
    "        eval_r = agent.evaluate(env, p_context=p_context[t,:], n_episodes=1)[0]\n",
    "        evaluation_rewards_for_this_rep.append(eval_r)\n",
    "\n",
    "    return np.stack(training_rewards_for_this_rep), np.stack(evaluation_rewards_for_this_rep)\n",
    "# -----------------------------------------\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "REWARDS_PREFIX = 'models/fig3a_rewards_'\n",
    "results = {}\n",
    "n_reps = 100\n",
    "n_processes = 17\n",
    "if train_3a:\n",
    "    for tag, inst, test_point in (('normal', False, None), ('new', True, 5000)):\n",
    "        if tag == 'new':\n",
    "            continue\n",
    "        arglist = [(data, p_contexts, inst, test_point) for _ in range(n_reps)]\n",
    "        #             ^--- we’ll thread through instantiate_from_average\n",
    "\n",
    "        running_mean = []                   # for live quality metric\n",
    "        all_results  = []\n",
    "\n",
    "        with mp.Pool(processes=n_processes) as pool:\n",
    "            # imap_unordered streams back as soon as a worker finishes\n",
    "            for res in tqdm(pool.imap_unordered(run_single_rep, arglist),\n",
    "                            total=n_reps,\n",
    "                            desc=f'Reps [{tag}]', \n",
    "                            dynamic_ncols=True):\n",
    "                all_results.append(res)\n",
    "\n",
    "                # -- optional live metric (mean of last 10 eval rewards) --\n",
    "                running_mean.append(np.mean(res[1]))  # res = (train, eval)\n",
    "                if len(running_mean) > 10:\n",
    "                    running_mean.pop(0)\n",
    "                tqdm.write(f'latest μ(eval) ≈ {np.mean(running_mean):.2f}')\n",
    "\n",
    "        # ---------- post-processing ----------\n",
    "        all_results_array                 = np.array(all_results)\n",
    "        results[f\"rewards_3a_{tag}\"]      = np.mean(all_results_array, axis=0)\n",
    "        np.save(f\"{REWARDS_PREFIX}{tag}.npy\", all_results_array)\n",
    "        print(f\"Saved → {REWARDS_PREFIX}{tag}.npy\")\n",
    "\n",
    "else:\n",
    "    # unchanged…\n",
    "    for tag in ('normal', 'new'):\n",
    "        path = f\"{REWARDS_PREFIX}{tag}.npy\"\n",
    "        if os.path.exists(path):\n",
    "            print(f\"Loaded {path}\")\n",
    "            results[f\"rewards_3a_{tag}\"] = np.mean(np.load(path), axis=0)\n",
    "        else:\n",
    "            print(f\"Missing {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoothen the data with simple moving average\n",
    "rewards_3a_normal = np.load(REWARDS_PREFIX+'normal.npy')\n",
    "def smoothen(data, window_size=5):\n",
    "    \"\"\"Smoothen the data with a spatial average.\"\"\"\n",
    "    val = np.convolve(data, np.ones(window_size)/window_size, mode='same')\n",
    "    return val\n",
    "\n",
    "smoothened_rewards_3a_normal = smoothen(results[\"rewards_3a_normal\"][0], window_size=1)\n",
    "smoothened_rewards_3a_new = smoothen(results[\"rewards_3a_new\"][0], window_size=1)\n",
    "\n",
    "# Normalise data for plotting\n",
    "smoothened_rewards_3a_normal = (smoothened_rewards_3a_normal - np.min(smoothened_rewards_3a_normal)) / (np.max(smoothened_rewards_3a_normal) - np.min(smoothened_rewards_3a_normal))\n",
    "smoothened_rewards_3a_new = (smoothened_rewards_3a_new - np.min(smoothened_rewards_3a_new)) / (np.max(smoothened_rewards_3a_new) - np.min(smoothened_rewards_3a_new))\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"Times New Roman\",\n",
    "    \"font.size\": 19,\n",
    "    \"axes.titlesize\": 19,\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"legend.fontsize\": 12,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16\n",
    "})\n",
    "plt.figure(figsize=(12.5, 3.75))\n",
    "plt.plot(smoothened_rewards_3a_normal, label='Normal COIN-Q', color='blue')\n",
    "plt.plot(smoothened_rewards_3a_new, label='Improved COIN-Q', color='orange')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Normalised Reward')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# Remove the top and right spines\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "plt.savefig('figures/fig3a_rewards.svg', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual State Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plot_utils import plot_context_probabilities\n",
    "\n",
    "data = np.concatenate([\n",
    "    np.ones((5, )),\n",
    "    0.5*np.ones((5, )),\n",
    "    np.nan*np.ones((5, )),\n",
    "    0.5*np.ones((5, )),\n",
    "    np.ones((5, )),\n",
    "])\n",
    "\n",
    "timescale = 11 # Conversion factor between COIN and training\n",
    "\n",
    "data = np.repeat(data, repeats=timescale, axis=0)\n",
    "\n",
    "# Run the COIN model for contextual estimates\n",
    "coin_model = COIN(\n",
    "    sigma_sensory_noise = 0.003, \n",
    "    sigma_motor_noise = 0.00182,\n",
    ")\n",
    "\n",
    "coin_model.perturbations = data[::timescale]\n",
    "output = coin_model.simulate_coin()\n",
    "\n",
    "\n",
    "known_c_resp, novel_c_resp = coin_model.get_responsibilities(output)\n",
    "p_contexts = np.concatenate([known_c_resp,novel_c_resp[:,None]],axis=-1) \n",
    "\n",
    "# Ensure COIN probabilities are of the right form\n",
    "# Novel context - should never have a NaN definition\n",
    "p_temp = p_contexts.copy()\n",
    "p_temp[np.isnan(p_temp)] = 0.0\n",
    "p_contexts[:,-1] = 1 - np.sum(p_temp[:,:-1], axis=1) # This variable has size [T,C]. It should be used for TRAINING, passed into the p_context parameter\n",
    "\n",
    "state_values = np.linspace(0.0, 1.5, 50)\n",
    "\n",
    "# Get predicted responsibilities\n",
    "pred_resp = coin_model.get_predicted_responsibilities(output, state_values)\n",
    "\n",
    "# Plotting - Not in Paper\n",
    "# Ensure interpolation of probabilities back to appropriate size\n",
    "p_contexts = np.repeat(p_contexts, repeats=timescale, axis=0)\n",
    "pred_resp = np.repeat(pred_resp, repeats=timescale, axis=1)\n",
    "plt.figure(figsize=(8, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(p_contexts)\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(data)\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(state_values, pred_resp[:,-1,:])\n",
    "plt.subplot(2,2,4)\n",
    "\n",
    "# Remove novel probability if feasible - sum of other terms non-zero\n",
    "idx = np.where(np.nansum(pred_resp[:,-1,:-1], axis=1) > 0)[0]\n",
    "pred_resp_new = pred_resp[:,-1,:].copy()\n",
    "pred_resp_new[idx,-1] = 0.0\n",
    "pred_resp_new[idx,:-1] = pred_resp_new[idx,:-1] / np.nansum(pred_resp_new[idx,:-1], axis=1)[:,None]\n",
    "plt.plot(state_values, pred_resp_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plot_utils import plot_context_probabilities, plot_hor_context_prob\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"Times New Roman\",\n",
    "    \"font.size\": 19,\n",
    "    \"axes.titlesize\": 19,\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"legend.fontsize\": 12,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16\n",
    "})\n",
    "\n",
    "ax1 = plot_hor_context_prob(\n",
    "    pred_resp,\n",
    "    state_values,\n",
    "    data,\n",
    "    train_stride=timescale,\n",
    "    min_intensity = 0.1,\n",
    "    fig_size=(8,8)\n",
    ")\n",
    "\n",
    "# Set ax1 label\n",
    "ax1.set_xlabel(\"Episode\")\n",
    "ax1.set_ylabel(\"Parameter Value\")\n",
    "\n",
    "plt.savefig('figures/fig3b_context_decay.svg', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COIN Freeze - Example\n",
    "from utils.plot_utils import plot_context_probabilities\n",
    "\n",
    "true_data = np.concatenate([\n",
    "    np.ones((5, )),\n",
    "    0.5*np.ones((5, )),\n",
    "    1.5*np.ones((5, )),\n",
    "    0.5*np.ones((5, )),\n",
    "    np.ones((5, )),\n",
    "])\n",
    "\n",
    "# Boolean mask to only keep changes in the data - this is a cheat to show the results.\n",
    "# A real code would, in real time, check if the current prediction is a novel or a different context that needs to be reinforced.\n",
    "coin_mask = np.concatenate([\n",
    "    np.ones((3, )), # Contexts being learned from here\n",
    "    np.zeros((2, )), \n",
    "    np.ones((3, )),\n",
    "    np.zeros((2, )),\n",
    "    np.ones((3, )),\n",
    "    np.zeros((2, )),\n",
    "    np.ones((1, )), # Contexts already formed, so only look at one value to reinforce (change of context)\n",
    "    np.zeros((4, )),\n",
    "    np.ones((1, )),\n",
    "    np.zeros((4, ))\n",
    "])\n",
    "\n",
    "# Coin does not see the whole of the above - it is frozen at the unseen parts\n",
    "coin_data = true_data.copy()\n",
    "coin_data = coin_data[coin_mask.astype(bool)]\n",
    "\n",
    "# Run the COIN model for contextual estimates - we raise the retention slightly from default\n",
    "coin_model = COIN(\n",
    "    sigma_sensory_noise = 0.003, \n",
    "    sigma_motor_noise = 0.00182,\n",
    "    prior_mean_retention=0.99,\n",
    ")\n",
    "\n",
    "coin_model.perturbations = coin_data\n",
    "output = coin_model.simulate_coin()\n",
    "\n",
    "\n",
    "known_c_resp, novel_c_resp = coin_model.get_responsibilities(output)\n",
    "p_contexts = np.concatenate([known_c_resp,novel_c_resp[:,None]],axis=-1) \n",
    "\n",
    "# Ensure COIN probabilities are of the right form\n",
    "# Novel context - should never have a NaN definition\n",
    "p_temp = p_contexts.copy()\n",
    "p_temp[np.isnan(p_temp)] = 0.0\n",
    "p_contexts[:,-1] = 1 - np.sum(p_temp[:,:-1], axis=1) # This variable has size [T,C]. It should be used for TRAINING, passed into the p_context parameter\n",
    "\n",
    "# State-values, include all values from 0.1 to 2.0\n",
    "state_values = np.arange(0.1, 2.0, 0.025)\n",
    "\n",
    "# Get predicted responsibilities\n",
    "pred_resp = coin_model.get_predicted_responsibilities(output, state_values) # Size [N,T,C]\n",
    "\n",
    "# Get pred_resp and p_contexts to the same size as data by repeating their last value\n",
    "N = coin_mask.size\n",
    "C = p_contexts.shape[1]\n",
    "n_states = pred_resp.shape[0]\n",
    "\n",
    "# 1) rebuild full p_contexts\n",
    "full_p = np.full((N, C), np.nan)\n",
    "full_p[coin_mask.astype(bool), :] = p_contexts\n",
    "\n",
    "# forward‐fill along time axis\n",
    "for t in range(1, N):\n",
    "    mask_nan = np.isnan(full_p[t])\n",
    "    full_p[t, mask_nan] = full_p[t-1, mask_nan]\n",
    "\n",
    "# now full_p is shape (N, C)\n",
    "\n",
    "# 2) rebuild full pred_resp\n",
    "full_pred = np.full((n_states, N, C), np.nan)\n",
    "full_pred[:, coin_mask.astype(bool), :] = pred_resp\n",
    "\n",
    "# forward‐fill along the time axis for each state/k\n",
    "for s in range(n_states):\n",
    "    for t in range(1, N):\n",
    "        nan_ctx = np.isnan(full_pred[s, t])\n",
    "        full_pred[s, t, nan_ctx] = full_pred[s, t-1, nan_ctx]\n",
    "\n",
    "# Replace your old variables\n",
    "p_contexts = full_p\n",
    "pred_resp  = full_pred\n",
    "\n",
    "# Plotting - Not in Paper\n",
    "# Ensure interpolation of probabilities back to appropriate size\n",
    "plt.figure(figsize=(8, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(p_contexts)\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(true_data)\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(state_values, pred_resp[:,-1,:])\n",
    "plt.subplot(2,2,4)\n",
    "\n",
    "# Remove novel probability if feasible - sum of other terms non-zero\n",
    "idx = np.where(np.nansum(pred_resp[:,-1,:-1], axis=1) > 0)[0]\n",
    "pred_resp_new = pred_resp[:,-1,:].copy()\n",
    "pred_resp_new[idx,-1] = 0.0\n",
    "pred_resp_new[idx,:-1] = pred_resp_new[idx,:-1] / np.nansum(pred_resp_new[idx,:-1], axis=1)[:,None]\n",
    "plt.plot(state_values, pred_resp_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plot_utils import plot_context_probabilities, plot_hor_context_prob\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"Times New Roman\",\n",
    "    \"font.size\": 19,\n",
    "    \"axes.titlesize\": 19,\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"legend.fontsize\": 12,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16\n",
    "})\n",
    "\n",
    "ax1 = plot_hor_context_prob(\n",
    "    pred_resp,\n",
    "    state_values,\n",
    "    true_data,\n",
    "    train_stride=1,\n",
    "    min_intensity = 0.1,\n",
    "    fig_size=(10,7),\n",
    "    legend=False\n",
    ")\n",
    "\n",
    "# Set ax1 label\n",
    "ax1.set_xlabel(\"Episode\")\n",
    "ax1.set_ylabel(\"Parameter Value\")\n",
    "plt.savefig('figures/fig3c_context_freeze.svg', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_rep(data, p_context, gen_val, pred_resp, param_name='length'):\n",
    "    \"\"\"\n",
    "    Runs one repetition of training across all data points. After every training step, evaluate for every \n",
    "    amplitude in gen_val.\n",
    "    Returns the list of rewards for the evaluation steps.\n",
    "    \"\"\"\n",
    "    # Imports for multiprocessing\n",
    "    from environments import CustomCartPoleEnv\n",
    "    from rl import COINPPOAgent\n",
    "    import numpy as np\n",
    "    from tqdm.notebook import tqdm\n",
    "\n",
    "    # Write data to context-prob dictionary form\n",
    "    data_dict = {}\n",
    "    J = p_context.shape[1]-1 # number of contexts (excluding novel)\n",
    "    for i in range(len(data)):\n",
    "        data_dict[i] = {}\n",
    "        for j in range(J):\n",
    "            data_dict[i][j+1] = p_context[i,j]\n",
    "        data_dict[i]['novel'] = p_context[i,-1]\n",
    "\n",
    "    # Also write pred_resp to dictionary form\n",
    "    pred_resp_dicts = []\n",
    "    for i in range(pred_resp.shape[0]):\n",
    "        pred_resp_dicts.append([])\n",
    "        for t in range(pred_resp.shape[1]):\n",
    "            pred_resp_dicts[i].append({})\n",
    "            for j in range(J):\n",
    "                pred_resp_dicts[i][t][j+1] = pred_resp[i,t,j]\n",
    "            pred_resp_dicts[i][t]['novel'] = pred_resp[i,t,-1]\n",
    "\n",
    "    kwargs = {param_name: 1.0}\n",
    "    env = CustomCartPoleEnv(render_mode=\"none\", **kwargs)\n",
    "    agent = COINPPOAgent(base_obs_dim=4, act_dim=2, ctx_ids=data_dict[0].keys())\n",
    "\n",
    "    rewards_for_this_rep = []\n",
    "    training_rewards_for_this_rep = []\n",
    "    evaluation_rewards_for_this_rep = []\n",
    "    for epoch, param_val in tqdm(enumerate(data), total=len(data)):\n",
    "        # Create the environment for each amplitude\n",
    "        kwargs = {param_name: param_val}\n",
    "        env = CustomCartPoleEnv(render_mode=\"none\", **kwargs)\n",
    "\n",
    "        # Set current context function\n",
    "        context_fcn = lambda i: data_dict[epoch]\n",
    "\n",
    "        r = agent.train_step(env, context_probs_fn=context_fcn) # Rollout steps here\n",
    "        mean_return = r[\"mean_episode_return\"]\n",
    "        training_rewards_for_this_rep.append(mean_return)\n",
    "\n",
    "        eval_r = agent.evaluate(env, context_probs_fn=context_fcn, n_episodes=1)[0]\n",
    "        evaluation_rewards_for_this_rep.append(eval_r)\n",
    "          \n",
    "        # Evaluate for all the gen_val\n",
    "        evaluation_rewards = np.zeros((gen_val.size,))\n",
    "\n",
    "        for i in range(evaluation_rewards.size):\n",
    "            kwargs = {param_name: gen_val[i]}\n",
    "            env = CustomCartPoleEnv(render_mode=\"none\", **kwargs)\n",
    "\n",
    "            context_fcn = lambda x: pred_resp_dicts[i][epoch]\n",
    "            evaluation_rewards[i] = np.mean(agent.evaluate(env, context_probs_fn=context_fcn, n_episodes=10, ignore_novel=True)[0])\n",
    "        rewards_for_this_rep.append(evaluation_rewards)\n",
    "\n",
    "    return np.stack(rewards_for_this_rep), np.stack(training_rewards_for_this_rep), np.stack(evaluation_rewards_for_this_rep)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REWARDS_PATH_LENGTH = 'models/fig3b_rewards.npy'\n",
    "train_3b = False\n",
    "if train_3b:\n",
    "    all_results_length = run_single_rep(true_data, p_contexts, state_values, pred_resp, param_name='length')\n",
    "\n",
    "    rewards_3b = all_results_length[0]\n",
    "\n",
    "    # Save results\n",
    "    np.save(REWARDS_PATH_LENGTH, rewards_3b)\n",
    "    print(f\"Training complete. Rewards saved to '{REWARDS_PATH_LENGTH}'.\")\n",
    "        \n",
    "else:\n",
    "    # Load the saved results from the training\n",
    "    if os.path.exists(REWARDS_PATH_LENGTH):\n",
    "        rewards_3b = np.load(REWARDS_PATH_LENGTH)\n",
    "        print(f\"Loaded rewards from '{REWARDS_PATH_LENGTH}'.\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No saved rewards found at '{REWARDS_PATH_LENGTH}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"Times New Roman\",\n",
    "    \"font.size\": 19,\n",
    "    \"axes.titlesize\": 19,\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"legend.fontsize\": 12,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16\n",
    "})\n",
    "\n",
    "# ── Normalize and mask zero entries ─────────────────────────────────────\n",
    "r = rewards_3b  # shape (N_epochs, B_params)\n",
    "r = (r - r.min()) / (r.max() - r.min())\n",
    "masked_r = np.ma.masked_where(r == 0, r)\n",
    "\n",
    "# Transpose so params→rows, epochs→cols\n",
    "masked_rt = masked_r.T  # now shape (B_params, N_epochs)\n",
    "N, B = masked_rt.shape  # N = B_params, B = N_epochs\n",
    "\n",
    "# ── Create colormap with black for masked (zero) values ────────────────\n",
    "cmap = plt.get_cmap('viridis').copy()\n",
    "cmap.set_bad(color='black')\n",
    "\n",
    "# ── Create figure and axes ─────────────────────────────────────────────\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "cax = ax.imshow(\n",
    "    masked_rt,\n",
    "    origin='lower',    # so param_idx=0 at bottom\n",
    "    aspect='auto',\n",
    "    interpolation='nearest',\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    cmap=cmap\n",
    ")\n",
    "\n",
    "# ── Highlight training data points ─────────────────────────────────────\n",
    "d = true_data\n",
    "# epoch indices along x-axis\n",
    "x_idx = np.arange(r.shape[0])\n",
    "# param indices along y-axis\n",
    "y_idx = np.rint(\n",
    "    (d - state_values.min()) /\n",
    "    (state_values.max() - state_values.min()) *\n",
    "    (state_values.size - 1)\n",
    ").astype(int)\n",
    "\n",
    "ax.scatter(\n",
    "    x_idx, y_idx,\n",
    "    facecolors='red',\n",
    "    edgecolors='white',\n",
    "    s=50,\n",
    "    linewidths=2,\n",
    "    marker='o'\n",
    ")\n",
    "\n",
    "# ── Labels, ticks, and colorbar ────────────────────────────────────────\n",
    "fig.colorbar(cax, ax=ax, label='Return (Scaled)')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Parameter Value')\n",
    "\n",
    "# tick locations\n",
    "ax.set_xticks(np.arange(0, r.shape[0], 5))\n",
    "ax.set_yticks(np.arange(0, state_values.size, 8))\n",
    "\n",
    "# tick labels\n",
    "ax.set_xticklabels(np.arange(0, len(true_data), 5))\n",
    "ax.set_yticklabels([f\"{float(v):.2f}\" for v in state_values[::8]])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/fig3b_generalisation.svg', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noisy Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- Contextual Comparison - Noisy vs Noiseless -------\n",
    "data = np.concatenate([\n",
    "    np.ones((5, )),\n",
    "    -np.ones((5, )),\n",
    "    np.zeros((5, )),\n",
    "    np.ones((5, ))\n",
    "])\n",
    "\n",
    "state_values = np.linspace(-1.5, 1.5, 50)\n",
    "\n",
    "# Noiseless COIN (small noise for algorithm to work)\n",
    "coin_model = COIN(\n",
    "    sigma_sensory_noise = 0.003, \n",
    "    sigma_motor_noise = 0.00182,\n",
    "    prior_mean_retention=0.99,\n",
    ")\n",
    "\n",
    "coin_model.perturbations = data\n",
    "output = coin_model.simulate_coin()\n",
    "\n",
    "# Responsibilities - for training\n",
    "known_c_resp, novel_c_resp = coin_model.get_responsibilities(output)\n",
    "p_contexts = np.concatenate([known_c_resp,novel_c_resp[:,None]],axis=-1)\n",
    "p_temp = p_contexts.copy()\n",
    "p_temp[np.isnan(p_temp)] = 0.0\n",
    "p_contexts[:,-1] = 1 - np.sum(p_temp[:,:-1], axis=1)\n",
    "\n",
    "# Predicted responsibilities - for generalisation tests\n",
    "pred_resp = coin_model.get_predicted_responsibilities(output, state_values)\n",
    "\n",
    "# Noisy COIN\n",
    "coin_model_noisy = COIN(\n",
    "    sigma_sensory_noise = 0.06, \n",
    "    sigma_motor_noise = 0.0182,\n",
    "    prior_mean_retention=0.99,\n",
    ")\n",
    "\n",
    "coin_model_noisy.perturbations = data\n",
    "output_noisy = coin_model_noisy.simulate_coin()\n",
    "\n",
    "# Get state feedback - True training values\n",
    "feedback_values = output_noisy[\"runs\"][0][\"state_feedback\"]\n",
    "\n",
    "# Responsibilities - for training\n",
    "known_c_resp_noisy, novel_c_resp_noisy = coin_model_noisy.get_responsibilities(output_noisy)\n",
    "p_contexts_noisy = np.concatenate([known_c_resp_noisy,novel_c_resp_noisy[:,None]],axis=-1)\n",
    "p_temp_noisy = p_contexts_noisy.copy()\n",
    "p_temp_noisy[np.isnan(p_temp_noisy)] = 0.0\n",
    "p_contexts_noisy[:,-1] = 1 - np.sum(p_temp_noisy[:,:-1], axis=1)\n",
    "\n",
    "# Predicted responsibilities - for generalisation tests\n",
    "pred_resp_noisy = coin_model_noisy.get_predicted_responsibilities(output_noisy, state_values)\n",
    "\n",
    "\n",
    "# Plotting - Not in Paper\n",
    "plt.figure(figsize=(8, 10))\n",
    "plt.plot(data, label='True Data', color='blue')\n",
    "plt.scatter(np.arange(feedback_values.size), feedback_values, alpha=0.9, edgecolor='none', color='red', s=100)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(p_contexts)\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(p_contexts_noisy)\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(state_values, pred_resp[:,-1,:])\n",
    "plt.subplot(2,2,4)\n",
    "plt.plot(state_values, pred_resp_noisy[:,-1,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plot_utils import plot_context_probabilities\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"Times New Roman\",\n",
    "    \"font.size\": 19,\n",
    "    \"axes.titlesize\": 19,\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"legend.fontsize\": 12,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16\n",
    "})\n",
    "\n",
    "ax1 = plot_context_probabilities(\n",
    "    pred_resp,\n",
    "    state_values,\n",
    "    data,\n",
    "    1,\n",
    "    min_intensity = 0.3,\n",
    "    fig_size=(6,6),\n",
    "    legend=True\n",
    ")\n",
    "plt.savefig('figures/fig3c_noiseless_probabilities_legend.svg', dpi=300, bbox_inches='tight')\n",
    "\n",
    "ax2 = plot_context_probabilities(\n",
    "    pred_resp_noisy,\n",
    "    state_values,\n",
    "    feedback_values,\n",
    "    1,\n",
    "    min_intensity = 0.3,\n",
    "    fig_size=(6,6),\n",
    "    legend=False\n",
    ")\n",
    "plt.savefig('figures/fig3c_noisy_probabilities.svg', dpi=300, bbox_inches='tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_rep(data, p_context, gen_val, pred_resp, param_name='length'):\n",
    "    \"\"\"\n",
    "    Runs one repetition of training across all data points. After every training step, evaluate for every \n",
    "    amplitude in gen_val.\n",
    "    Returns the list of rewards for the evaluation steps.\n",
    "    \"\"\"\n",
    "    # Imports for multiprocessing\n",
    "    from environments import CustomCartPoleEnv\n",
    "    from rl import COINPPOAgent\n",
    "    import numpy as np\n",
    "    from tqdm.notebook import tqdm\n",
    "\n",
    "    # Write data to context-prob dictionary form\n",
    "    data_dict = {}\n",
    "    J = p_context.shape[1]-1 # number of contexts (excluding novel)\n",
    "    for i in range(len(data)):\n",
    "        data_dict[i] = {}\n",
    "        for j in range(J):\n",
    "            data_dict[i][j+1] = p_context[i,j]\n",
    "        data_dict[i]['novel'] = p_context[i,-1]\n",
    "\n",
    "    # Also write pred_resp to dictionary form\n",
    "    pred_resp_dicts = []\n",
    "    for i in range(pred_resp.shape[0]):\n",
    "        pred_resp_dicts.append([])\n",
    "        for t in range(pred_resp.shape[1]):\n",
    "            pred_resp_dicts[i].append({})\n",
    "            for j in range(J):\n",
    "                pred_resp_dicts[i][t][j+1] = pred_resp[i,t,j]\n",
    "            pred_resp_dicts[i][t]['novel'] = pred_resp[i,t,-1]\n",
    "\n",
    "    kwargs = {param_name: 1.0}\n",
    "    env = CustomCartPoleEnv(render_mode=\"none\", **kwargs)\n",
    "    agent = COINPPOAgent(base_obs_dim=4, act_dim=2, ctx_ids=data_dict[0].keys())\n",
    "\n",
    "    rewards_for_this_rep = []\n",
    "    training_rewards_for_this_rep = []\n",
    "    evaluation_rewards_for_this_rep = []\n",
    "    for epoch, param_val in tqdm(enumerate(data), total=len(data)):\n",
    "        # Create the environment for each amplitude\n",
    "        kwargs = {param_name: param_val}\n",
    "        env = CustomCartPoleEnv(render_mode=\"none\", **kwargs)\n",
    "\n",
    "        # Set current context function\n",
    "        context_fcn = lambda i: data_dict[epoch]\n",
    "\n",
    "        r = agent.train_step(env, context_probs_fn=context_fcn) # Rollout steps here\n",
    "        mean_return = r[\"mean_episode_return\"]\n",
    "        training_rewards_for_this_rep.append(mean_return)\n",
    "\n",
    "        eval_r = agent.evaluate(env, context_probs_fn=context_fcn, n_episodes=1)[0]\n",
    "        evaluation_rewards_for_this_rep.append(eval_r)\n",
    "          \n",
    "        # Evaluate for all the gen_val\n",
    "        evaluation_rewards = np.zeros((gen_val.size,))\n",
    "\n",
    "        for i in range(evaluation_rewards.size):\n",
    "            kwargs = {param_name: gen_val[i]}\n",
    "            env = CustomCartPoleEnv(render_mode=\"none\", **kwargs)\n",
    "\n",
    "            context_fcn = lambda x: pred_resp_dicts[i][epoch]\n",
    "            evaluation_rewards[i] = np.mean(agent.evaluate(env, context_probs_fn=context_fcn, n_episodes=10, ignore_novel=True)[0])\n",
    "        rewards_for_this_rep.append(evaluation_rewards)\n",
    "\n",
    "    return np.stack(rewards_for_this_rep), np.stack(training_rewards_for_this_rep), np.stack(evaluation_rewards_for_this_rep)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REWARDS_PATH_NOISELESS = 'models/fig3b_rewards_noiseless.npy'\n",
    "REWARDS_PATH_NOISY = 'models/fig3b_rewards_noisy.npy'\n",
    "train_3c = False\n",
    "if train_3c:\n",
    "    all_results_noiseless = run_single_rep(data, p_contexts, state_values, pred_resp, param_name='gravity')\n",
    "\n",
    "    rewards_noiseless = all_results_noiseless[0]\n",
    "\n",
    "    # Save results\n",
    "    np.save(REWARDS_PATH_NOISELESS, rewards_noiseless)\n",
    "    print(f\"Training complete. Rewards saved to '{REWARDS_PATH_NOISELESS}'.\")\n",
    "\n",
    "    all_results_noisy = run_single_rep(feedback_values, p_contexts_noisy, state_values, pred_resp_noisy, param_name='gravity')\n",
    "    rewards_noisy = all_results_noisy[0]\n",
    "\n",
    "    # Save results\n",
    "    np.save(REWARDS_PATH_NOISY, rewards_noisy)\n",
    "    print(f\"Training complete. Rewards saved to '{REWARDS_PATH_NOISY}'.\")\n",
    "        \n",
    "else:\n",
    "    # Load the saved results from the training\n",
    "    if os.path.exists(REWARDS_PATH_NOISELESS):\n",
    "        rewards_noiseless = np.load(REWARDS_PATH_NOISELESS)\n",
    "        print(f\"Loaded rewards from '{REWARDS_PATH_NOISELESS}'.\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No saved rewards found at '{REWARDS_PATH_NOISELESS}'.\")\n",
    "    \n",
    "    if os.path.exists(REWARDS_PATH_NOISY):\n",
    "        rewards_noisy = np.load(REWARDS_PATH_NOISY)\n",
    "        print(f\"Loaded rewards from '{REWARDS_PATH_NOISY}'.\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No saved rewards found at '{REWARDS_PATH_NOISY}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ─── Global style ───────────────────────────────────────────────────────────\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"Times New Roman\",\n",
    "    \"font.size\": 19,\n",
    "    \"axes.titlesize\": 19,\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"legend.fontsize\": 12,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16\n",
    "})\n",
    "\n",
    "def plot_return_heatmap(\n",
    "    rewards: np.ndarray,\n",
    "    feedback: np.ndarray,\n",
    "    state_values: np.ndarray,\n",
    "    xtick_step: int,\n",
    "    ytick_step: int,\n",
    "    filename: str,\n",
    "    title: str = \"\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot a heatmap of scaled returns over epochs vs. parameter values,\n",
    "    with masked zeros shown in black and training feedback overlaid.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    rewards : array, shape (n_epochs, n_params)\n",
    "        Raw return values for each epoch/parameter.\n",
    "    feedback : array, shape (n_epochs,)\n",
    "        The “true” parameter value at each epoch (for scatter overlay).\n",
    "    state_values : array, shape (n_params,)\n",
    "        The full sweep of parameter values.\n",
    "    xtick_step : int\n",
    "        Step between epoch tick marks.\n",
    "    ytick_step : int\n",
    "        Step between parameter-value tick marks.\n",
    "    filename : str\n",
    "        Path under which to save the figure (SVG format).\n",
    "    title : str, optional\n",
    "        Figure title (displayed at top of axes).\n",
    "    \"\"\"\n",
    "    # 1. normalize and mask zeros\n",
    "    norm = (rewards - rewards.min()) / (rewards.max() - rewards.min())\n",
    "    masked = np.ma.masked_where(norm == 0, norm)\n",
    "\n",
    "    # 2. transpose so params→rows, epochs→cols\n",
    "    heat = masked.T  # shape (n_params, n_epochs)\n",
    "    n_params, n_epochs = heat.shape\n",
    "\n",
    "    # 3. prepare colormap\n",
    "    cmap = plt.get_cmap(\"viridis\").copy()\n",
    "    cmap.set_bad(color=\"black\")\n",
    "\n",
    "    # 4. plot\n",
    "    fig, ax = plt.subplots(figsize=(7, 7))\n",
    "    im = ax.imshow(\n",
    "        heat,\n",
    "        origin=\"lower\",\n",
    "        aspect=\"auto\",\n",
    "        interpolation=\"nearest\",\n",
    "        vmin=0, vmax=1,\n",
    "        cmap=cmap\n",
    "    )\n",
    "\n",
    "    # 5. overlay feedback points\n",
    "    x_idx = np.arange(n_epochs)\n",
    "    y_idx = np.clip(\n",
    "        np.rint((feedback - state_values.min()) /\n",
    "                (state_values.max() - state_values.min()) *\n",
    "                (n_params - 1)).astype(int),\n",
    "        0, n_params - 1\n",
    "    )\n",
    "    ax.scatter(\n",
    "        x_idx, y_idx,\n",
    "        facecolors=\"red\",\n",
    "        edgecolors=\"white\",\n",
    "        s=50, linewidths=2, marker=\"o\"\n",
    "    )\n",
    "\n",
    "    # 6. labels, ticks, colorbar\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Parameter Value\")\n",
    "\n",
    "    ax.set_xticks(np.arange(0, n_epochs, xtick_step))\n",
    "    ax.set_xticklabels(np.arange(0, rewards.shape[0], xtick_step))\n",
    "\n",
    "    ax.set_yticks(np.arange(0, n_params, ytick_step))\n",
    "    ax.set_yticklabels([f\"{v:.2f}\" for v in state_values[::ytick_step]])\n",
    "\n",
    "    fig.colorbar(im, ax=ax, label=\"Return (scaled)\")\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(filename, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ─── Usage ─────────────────────────────────────────────────────────────────\n",
    "\n",
    "# 1) Noiseless rewards\n",
    "plot_return_heatmap(\n",
    "    rewards=rewards_noiseless,\n",
    "    feedback=data,\n",
    "    state_values=state_values,\n",
    "    xtick_step=5,\n",
    "    ytick_step=8,\n",
    "    filename=\"figures/fig3c_noiseless_generalisation.svg\",\n",
    ")\n",
    "\n",
    "# 2) Noisy rewards\n",
    "plot_return_heatmap(\n",
    "    rewards=rewards_noisy,\n",
    "    feedback=feedback_values,\n",
    "    state_values=state_values,\n",
    "    xtick_step=5,\n",
    "    ytick_step=8,\n",
    "    filename=\"figures/fig3c_noisy_generalisation.svg\",\n",
    ")\n",
    "\n",
    "# Print maximum rewards - For normalisation verification (If both are 500, then normalisation can be compared without bias)\n",
    "print(\"Noiseless max reward:\", rewards_noiseless.max())\n",
    "print(\"Noisy max reward:\", rewards_noisy.max())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
